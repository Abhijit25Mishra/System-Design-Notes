{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Compilation of all my learning. All about distributed systems, software engineering and computer science.</p> <p>Feel free to refer to my notes for your own use. </p>"},{"location":"Architectural%20choice/","title":"Monolithic vs. Microservices","text":""},{"location":"Architectural%20choice/#monolithic","title":"Monolithic","text":"<ul> <li>A single, unified codebase.</li> <li>All business functions are contained in a single application.</li> <li>All components are interdependent.</li> <li>The application is deployed as a single unit.</li> </ul> <p>Pros: 1. Better for small-scale teams. 2. Simplicity and speed. Development is faster and deployment is straightforward. 3. Tracing a request is easier.</p> <p>Cons: As it scales, all the pros turn into cons. 1. Slow development, unintended changes. 2. Scaling becomes inefficient. 3. A single error can compromise the availability of the entire application. 4. New technology adoption is blocked.</p>"},{"location":"Architectural%20choice/#microservices","title":"Microservices","text":"<ul> <li>A set of applications.</li> <li>Independent of each other.</li> <li>Loosely coupled, i.e., each service handles a different business function.</li> <li>Communicates through APIs.</li> </ul> <p>Pros: 1. Distinct scaling requirements can be handled. 2. Enables technological diversity. 3. Dramatically improves fault isolation.</p> <p>Microservices solve a lot of problems that are present in a monolithic architecture. Adopting microservices is often driven by the need to scale the organization, not just the application. A large team working on a single monolithic codebase inevitably faces high coordination overhead, frequent merge conflicts, and a bottlenecked release pipeline. Microservices minimize cross-team communication overhead, enabling parallel development and deployment. Data consistency is one area where a monolithic architecture is often preferred, as it is easier to maintain strong consistency.</p> <p>[[When to create or extend a service|  when to create or extend microservcies]]</p> Dimension Monolithic Architecture Microservices Architecture Team Size Ideal for small teams (&lt;10-15 developers) where communication is simple. Suitable for larger organizations with multiple teams that need to work independently. Development Speed High initial velocity due to simplicity. Slower long-term velocity as complexity grows. Slower initial setup due to distributed system complexity. Higher long-term velocity for large teams. Scalability Coarse-grained. The entire application must be scaled, even if only one component is a bottleneck. Fine-grained. Each service can be scaled independently, optimizing resource usage. Deployment Simple. A single artifact is deployed. Complex. Requires mature automation (CI/CD) and container orchestration for many services. Operational Overhead Low. Fewer moving parts to manage, monitor, and secure. High. Requires managing a distributed system, including service discovery, distributed tracing, and complex networking. Data Consistency Strong consistency is easily achieved with a single, transactional database (ACID). Eventual consistency is the default. Achieving strong consistency across services is complex and requires patterns like Sagas. Fault Isolation Low. An error in one module can bring down the entire application. High. Failure in one service is isolated and typically does not cascade to the entire system. Technology Stack Homogeneous. Constrained by the technologies chosen at the outset. Heterogeneous (Polyglot). Each service can use the most appropriate language, framework, and database."},{"location":"Database%20per%20Service%20pattern/","title":"Database per Service pattern","text":"<p>The Database per Service pattern is a core tenet of microservices architecture which dictates that each microservice must own and manage its own private database (or a private schema within a shared database server). Crucially, this data store is considered part of the service's internal implementation and must not be directly accessed by any other service. All communication and data exchange must occur exclusively through the service's public API.</p> <p>Pros 1. Loose Coupling 2. Technology Freedom (Polyglot Persistence) 3. Independent Scalability and Fault Isolation</p> <p>Cons: 1. Distributed Transactions: Implementing business transactions that span multiple services becomes highly complex. Traditional ACID (Atomicity, Consistency, Isolation, Durability) transactions, which rely on two-phase commits, are often not feasible or supported by modern NoSQL databases and are generally avoided in microservices due to their negative impact on availability and performance, as described by the CAP theorem. 2. Cross-Service Queries: Performing queries that require joining data from multiple, now-separate databases is challenging. Such operations cannot be handled by a simple SQL join and require more complex patterns. 3. Operational Complexity: The need to deploy, manage, monitor, and back up a multitude of different database technologies increases the operational burden on the organization.</p> <p>To address the challenge of distributed transactions, the [[SAGA pattern]] is the most common solution.</p>"},{"location":"Domain%20Driven%20Design/","title":"Domain Driven Design","text":"<p>when a new microservice is being created. the most crucial task is to define its boundaries. A microservices value is directly tied to its autonomy and single minded focus. </p> <p>Poorly defined microservices lead to systemic problems, like overly chatty, data consistency issues, distributed monoliths.</p>"},{"location":"Domain%20Driven%20Design/#why-ddd-is-essential","title":"why DDD is essential ?","text":"<p>DDD is a software design methodology that advocates for modeling software based on the underlying business domain. Its core premise is that complex business logic should be at the heart of the application, and the software's structure should reflect the real-world processes and rules of the business. </p> <p>This philosophy aligns perfectly with the goals of microservices architecture. Microservices are intended to be organized around business capabilities, ensuring they have high functional cohesion (doing one thing well) and loose coupling (minimal dependency on other services).</p> <p>DDD provides the tools to achieve this. It helps decompose a large, complex system into self-contained, understandable units by systematically analyzing business requirements.</p>"},{"location":"Domain%20Driven%20Design/#strategic-ddd","title":"Strategic DDD","text":"<p>Focuses on high-level, large-scale, structure of the system. It is a collaborative process that involves both software developers and domain experts (such as business analysts and product managers) to map out the problem space before designing the solution space.</p> <ol> <li>Ubiquitous Language: A cornerstone of DDD is the development of a Ubiquitous Language a common, rigorous, and unambiguous vocabulary shared by all stakeholders.</li> <li>Bounded Context: The central pattern in Strategic DDD is the Bounded Context. A Bounded Context is a conceptual boundary within which a specific domain model and its Ubiquitous Language are valid, consistent, and unambiguous. DDD recognizes that attempting to create a single, unified model for an entire large-scale system is not feasible or cost-effective, as different parts of a business often use the same terms to mean subtly different things.</li> <li>In a microservices architecture, each Bounded Context is a prime candidate to become a microservice. This alignment ensures that each service owns a specific, well-defined part of the business domain, operates autonomously, and maintains its own internal consistency. Defining these boundaries is the most critical step; misdefined boundaries are a primary cause of tight coupling and architectural complexity.</li> <li>Context Maps: To manage the interactions between these boundaries, DDD uses Context Maps. These are diagrams that visualize the relationships between different Bounded Contexts, clarifying how the corresponding microservices will integrate.</li> </ol>"},{"location":"Domain%20Driven%20Design/#tactical-ddd","title":"Tactical DDD","text":"<p>Tactical DDD provides a set of design patterns for building the rich domain model inside a single Bounded Context. These patterns help structure the business logic in a way that is clean, maintainable, and decoupled from infrastructure concerns.</p> <ol> <li>Entities: These are objects defined not by their attributes, but by their thread of continuity and a distinct, persistent identity.</li> <li>Value Objects: These are immutable objects defined by their attributes, lacking a conceptual identity.</li> <li>Aggregates: An Aggregate is a cluster of associated objects (Entities and Value Objects) that are treated as a single unit for the purpose of data changes. Each Aggregate has a root entity, known as the Aggregate Root, which is the sole entry point for any command that modifies the Aggregate. This structure is crucial for enforcing business rules and invariants, as the Aggregate Root ensures that the entire cluster of objects remains in a consistent state after any operation.</li> </ol> <p>Aggregates are particularly important for microservice design. They represent a natural consistency boundary, meaning a single transaction should ideally not span multiple Aggregates. This concept maps directly to microservices, where Aggregates are often excellent candidates for the resources exposed by a service's API.</p>"},{"location":"Domain%20Driven%20Design/#data-consistency-issue","title":"Data Consistency Issue","text":"<p>Once a microservice's boundaries have been defined using Domain-Driven Design, the next logical step is to determine its data management strategy. The architectural choice to isolate services necessitates a corresponding isolation of their data. This leads to the adoption of the [[Database per Service pattern]], a foundational element of microservice design. While this pattern provides significant benefits in terms of autonomy and loose coupling, it introduces the profound challenge of maintaining data consistency and integrity in a distributed environment.</p>"},{"location":"Event%20Sourcing/","title":"Event Sourcing","text":"<p>it is a pattern for data management int distributed systems. </p> <p>Instead of storing only the current state of an entity, Event Sourcing stores the full history of changes as a sequence of immutable events. The current state of an entity is reconstructed by replaying these events in order.</p>"},{"location":"Event%20Sourcing/#will-write-more-about-this-later","title":"WILL WRITE MORE ABOUT THIS LATER","text":""},{"location":"Inter-Service%20Communication/","title":"Inter Service Communication","text":"<p>The way microservices communicate is a defining characteristic of the [[Architectural choice|architecture]], directly influencing performance, coupling, and resilience. A well-designed system often employs a mix of communication styles, choosing the right protocol for each specific interaction.</p> <p>there are mainly 2 different types of communication 1. Synchronous Communication: Direct Request/Response     1. REST over HTTP (Most used) : simple, less complexity, json, stateless, wide support, readable, bad performance.      2. gRPC : more complex, [[protobuf]], binary format (not readable), great performace  2. Asynchronous Communication: Event-Driven Architecture     1. Message Queues (RabbitMQ, Amazon SQS): This model is typically used for command-based communication, where a message represents a task to be performed. A producer sends a message to a specific queue, and a single consumer from a group of workers retrieves and processes that message. This pattern is excellent for distributing workloads, ensuring reliable task processing, and implementing load leveling to smooth out traffic spikes.     2. Event Streams / Publish - Suscribe (Apache kafka, Amazon SNS) : This model is the foundation of event-driven architectures and is used for broadcasting information. A producer service publishes an event\u2014a record of something that has happened\u2014to a topic. Any number of consumer services can subscribe to that topic and will receive a copy of the event to react to it independently. This pattern is ideal for notifying multiple parts of a system about a state change, such as an OrderPlaced or UserRegistered event. </p> <p>Asynchronous communication significantly improves system resilience and scalability. If a consumer service is temporarily unavailable, messages can be buffered in the broker and processed once the service recovers. This prevents failures from cascading. Furthermore, producers and consumers can be scaled independently based on their respective loads, and the non-blocking nature of the communication enhances the perceived responsiveness of the system. The main challenges are the added operational complexity of managing a message broker and the cognitive shift required to design and debug systems based on eventual consistency.</p> <p>A mature microservice ecosystem is rarely built on a single communication style. Instead, it is a hybrid, where the choice of protocol is made on a link-by-link basis, tailored to the specific requirements of each interaction.</p> <p>Table 1: Synchronous vs. Asynchronous Communication Trade-offs</p> Dimension Synchronous Communication Asynchronous Communication Coupling High temporal coupling. Client and server must be available simultaneously. Low coupling. Producer and consumer operate independently and do not need to be available at the same time. Latency Perceived latency is higher for the client, as it must block and wait for a response. Perceived latency is lower for the client (producer), as it does not wait for the operation to complete. Fault Tolerance Lower. A failure in the server service directly impacts the client service. Can lead to cascading failures. Higher. A message broker can buffer messages if a consumer is down, allowing it to recover and process them later. Scalability More limited. Scalability is constrained by the ability of services to handle concurrent requests in real-time. Higher. Producers and consumers can be scaled independently based on message volume and processing time. Implementation Complexity Simpler to reason about and implement, as it follows a familiar request-response flow. More complex. Requires managing a message broker, handling eventual consistency, and debugging distributed workflows. <p>Table 2: Comparison of REST, gRPC, and Message Queues</p> Feature REST over HTTP gRPC Message Queues / Event Streams Communication Style Synchronous Synchronous Asynchronous Performance Moderate. Text-based (JSON) and HTTP/1.1 overhead. High. Binary format (Protobuf) and HTTP/2 transport. Varies. High throughput but introduces broker latency. Decouples processing time from request time. Data Format JSON (human-readable) Protocol Buffers (binary, not human-readable). Any format (JSON, Avro, Protobuf), managed by producer/consumer. Contract Enforcement Loose. Often relies on documentation (e.g., OpenAPI/Swagger). Strict. Contract-first via .proto files, enabling code generation and type safety. Varies. Can be strict with a schema registry (e.g., for Avro/Protobuf) or loose. Coupling Loosely coupled at the implementation level, but tightly coupled temporally. Tightly coupled at the contract level (client/server need .proto file) and temporally. Loosely coupled. Producer and consumer are fully decoupled. Developer Experience Simple and familiar. Excellent tooling and browser support.46 Steeper learning curve. Requires specialized tooling for debugging. Limited browser support. Complex. Requires understanding of messaging concepts and managing a broker. Debugging is challenging. Typical Use Case Public-facing APIs, simple request-response interactions. High-performance internal service-to-service communication, streaming applications. Background jobs, event notification, decoupling services, data streaming pipelines."},{"location":"SAGA%20pattern/","title":"SAGA pattern","text":"<p>To address the challenge of distributed transactions, the Saga pattern is the most common solution for managing data consistency across services. A saga is a sequence of local transactions that are coordinated to execute a complete business process. Each step in the saga is a local, atomic transaction within a single service. Upon successful completion, it triggers the next step in the sequence, typically by publishing an event or sending a command.</p> <p>The key to the Saga pattern's atomicity is its handling of failures. If any local transaction in the sequence fails, the saga executes a series of compensating transactions to undo the work of the preceding, successfully completed steps. For example, if a CreateOrder saga involves ProcessPayment, UpdateInventory, and CreateShipment, and the CreateShipment step fails, compensating transactions for RefundPayment and RestoreInventory would be executed. This approach does not provide the immediate consistency of an ACID transaction but instead achieves eventual consistency, where the system converges to a consistent state over time.</p> <p>There are two primary approaches to implementing sagas:</p> <ol> <li> <p>Choreography: This is a decentralized approach where services communicate directly with each other by publishing and subscribing to events. There is no central coordinator.</p> </li> <li> <p>Orchestration: This is a centralized approach where a dedicated orchestrator service is responsible for managing the entire saga. The orchestrator sends commands to each participant service, waits for a reply, and decides the next step in the workflow. If a step fails, the orchestrator is responsible for triggering the necessary compensating transactions. This approach is easier to monitor and manage for complex workflows but introduces the orchestrator as a potential single point of failure and creates a degree of coupling between the participant services and the orchestrator.</p> </li> </ol>"},{"location":"When%20to%20create%20or%20extend%20a%20service/","title":"When to create or extend a service","text":"<p>There is no universal answer. It must be based on care evaluation of architectural principles and long term consequences. </p>"},{"location":"When%20to%20create%20or%20extend%20a%20service/#reasons-to-create-a-new-service","title":"Reasons to create a new service","text":"<ol> <li>Divergent Responsibilities and the Single Responsibility Principle</li> <li>Independent Scaling Requirements</li> <li>Different Technology Stack</li> <li>Team Ownership and Autonomy</li> </ol>"},{"location":"When%20to%20create%20or%20extend%20a%20service/#reasons-to-extend-a-new-service","title":"Reasons to extend a new service","text":"<ol> <li>High Cohesion and Domain Proximity</li> <li>Avoiding Premature Decomposition : every new service that gets introduces operational overhead, including deployment pipelines, monitoring, security configurations, and network communication latency.</li> </ol> <p>This decision-making process is fundamentally about preserving the [[Architectural choice|architectural integrity]] of the system by protecting the boundaries of each service.</p> <p>The first step when created a new microservice starts with [[Domain Driven Design]].</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/","title":"ProtoBuf Workflow","text":"<p>[[ProtoBuf]] workflow fundamentally shifts development from a \"code-first\" to a \"contract-first\" paradigm, a change with significant implications for team collaboration and system architecture.</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/#proto-file","title":".proto file","text":"<p>The entire Protobuf workflow originates from a single artifact: the .proto file. This plain text file serves as the definitive contract, or schema, for the data being exchanged. It contains language-agnostic definitions of data structures, known as messages, and optionally, the interfaces for remote services, known as services. This file is the central point of agreement between different components of a distributed system, such as a client and a server. It is managed as a source code asset, version-controlled, and serves as the single source of truth for the API contract.</p> <p>The basic structure of a .proto file includes several key components: - Syntax/Edition Declaration: The first non-comment line must declare the version of the Protobuf language being used, for example, syntax = \"proto3\"; or edition = \"2023\";. - Package Declaration: A package declaration (e.g., package tutorial;) helps prevent naming conflicts between different projects and provides a namespace. - Options: Language-specific option directives can be used to control how code is generated. For instance, option java_package = \"com.example.tutorial\"; specifies the Java package for the generated classes. - Message Definitions: The core of the file, where message blocks define the structure of the data, including field names, types, and unique numbers.</p> <p>By centralizing the API definition in this way, the .proto file becomes the focal point for design reviews and collaboration. Teams can agree on the contract before writing any implementation code, which decouples the interface from the implementation and facilitates parallel development. This \"contract-first\" approach significantly reduces integration errors and creates a system that is inherently self-documenting, with the .proto file acting as formal, machine-readable documentation.</p>"},{"location":"ProtoBuf/ProtoBuf/","title":"ProtoBuf","text":"<p>Originated within Google as an internal project. The primary motivation was to create a more efficient and robust mechanism for communication between Google's vast number of internal services. Full form is \"Protocol Buffers\".</p> <p>XML was a common choice for data interchange but its performance characteristics were inadequate for scale and latency requirement of Google's infrastructure. JSON gained popularity for its simplicity and direct mapping to JavaScript objects, but as a text-based format, it still carries significant size and parsing overhead compared to optimized binary formats.</p> <p>All data structure in protobuf are called messages.</p>"},{"location":"ProtoBuf/ProtoBuf/#core-philosophy","title":"Core Philosophy","text":"<p>The design of Protocol Buffer is built upon a distinct philosophy that prioritizes machine to machine efficiency and contract based design over given readability and ad hoc flexibility. This philosophy is embodied in three core principles.</p> <ol> <li> <p>Schema-Driven :  messages must be explicitly defined in special files with a dot protobuff extension These files are written using Interface Definition Language that describes the fields of a message their data type and their unique identifying numbers. This schema first approach enforces a strict contract between communicating parties ensuring type safety and data consistency. This contract becomes the single source of truth for the data structure</p> </li> <li> <p>language-agnostic and platform-neutral : The .proto file serves as a universal, language-independent contract. From this single definition, the Protobuf compiler, protoc, generates native source code for a wide array of programming languages, including C++, Java, Python, Go, C#, Dart, and Ruby. The generated code provides a strongly-typed, easy-to-use API for creating, manipulating, serializing, and deserializing the defined messages. This allows services written in different languages to communicate seamlessly, as long as they are compiled from the same .proto definition.</p> </li> <li> <p>performance-oriented : Design is optimized for speed and compactness. The format is a dense binary representation which is significantly smaller than the equivalent text based format like jason or XML Instead of using human readable fields in the payload protobuff uses unique integer tags to identify fields further reducing size It employs efficient encoding techniques such as variable length integers to represent numbers compactly the combination of these technique results in smaller payload which consume less of everything and results in faster serialization and deserialization.</p> </li> </ol>"},{"location":"ProtoBuf/ProtoBuf/#evolution-of-protobuf","title":"Evolution of Protobuf","text":"Feature proto2 Behavior proto3 Behavior edition = \"2023\" Behavior Syntax Declaration syntax = \"proto2\"; (or omitted) syntax = \"proto3\"; edition = \"2023\"; Field Cardinality required, optional, repeated Singular (implicitly optional), repeated Singular (explicit presence by default), repeated required Keyword Supported, but strongly discouraged for new designs. Removed. Use of required is a compile-time error. Not supported. Migrated required fields use a special feature set. Field Presence Explicit presence for optional fields. A has_ method is generated to check if a field was set. Implicit presence for scalar fields by default (cannot distinguish unset from default value). optional keyword was later added to enable explicit presence. Explicit presence is the default for all singular fields, providing clear tracking of whether a field is set. Default Values Custom default values can be specified using [default = value]. Custom default values are not supported. Fields default to a type-specific zero value (0, empty string, false). Explicit default values are reintroduced. Enums The first enum value can be non-zero. The first enum value must be zero, which serves as the default. The first enum value must be zero. Extensions Natively supported as a core feature. Not supported (though Any provides an alternative). Reintroduced as a feature."}]}