{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Compilation of all my learning. All about distributed systems, software engineering and computer science.</p> <p>Feel free to refer to my notes for your own use. </p>"},{"location":"Architectural%20choice/","title":"Architectural choice","text":""},{"location":"Architectural%20choice/#monolithic-vs-microservices","title":"Monolithic vs. Microservices","text":""},{"location":"Architectural%20choice/#monolithic","title":"Monolithic","text":"<ul> <li>A single, unified codebase.</li> <li>All business functions are contained in a single application.</li> <li>All components are interdependent.</li> <li>The application is deployed as a single unit.</li> </ul> <p>Pros:</p> <ol> <li> <p>Better for small-scale teams.</p> </li> <li> <p>Simplicity and speed. Development is faster and deployment is straightforward.</p> </li> <li> <p>Tracing a request is easier.</p> </li> </ol> <p>Cons: As it scales, all the pros turn into cons.</p> <ol> <li> <p>Slow development, unintended changes.</p> </li> <li> <p>Scaling becomes inefficient.</p> </li> <li> <p>A single error can compromise the availability of the entire application.</p> </li> <li> <p>New technology adoption is blocked.</p> </li> </ol>"},{"location":"Architectural%20choice/#microservices","title":"Microservices","text":"<ul> <li>A set of applications.</li> <li>Independent of each other.</li> <li>Loosely coupled, i.e., each service handles a different business function.</li> <li>Communicates through APIs.</li> </ul> <p>Pros:</p> <ol> <li> <p>Distinct scaling requirements can be handled.</p> </li> <li> <p>Enables technological diversity.</p> </li> <li> <p>Dramatically improves fault isolation.</p> </li> </ol> <p>Microservices solve a lot of problems that are present in a monolithic architecture. Adopting microservices is often driven by the need to scale the organization, not just the application. A large team working on a single monolithic codebase inevitably faces high coordination overhead, frequent merge conflicts, and a bottlenecked release pipeline. Microservices minimize cross-team communication overhead, enabling parallel development and deployment. Data consistency is one area where a monolithic architecture is often preferred, as it is easier to maintain strong consistency.</p> <p>[[When to create or extend a service]] ?</p> Dimension Monolithic Architecture Microservices Architecture Team Size Ideal for small teams (&lt;10-15 developers) where communication is simple. Suitable for larger organizations with multiple teams that need to work independently. Development Speed High initial velocity due to simplicity. Slower long-term velocity as complexity grows. Slower initial setup due to distributed system complexity. Higher long-term velocity for large teams. Scalability Coarse-grained. The entire application must be scaled, even if only one component is a bottleneck. Fine-grained. Each service can be scaled independently, optimizing resource usage. Deployment Simple. A single artifact is deployed. Complex. Requires mature automation (CI/CD) and container orchestration for many services. Operational Overhead Low. Fewer moving parts to manage, monitor, and secure. High. Requires managing a distributed system, including service discovery, distributed tracing, and complex networking. Data Consistency Strong consistency is easily achieved with a single, transactional database (ACID). Eventual consistency is the default. Achieving strong consistency across services is complex and requires patterns like Sagas. Fault Isolation Low. An error in one module can bring down the entire application. High. Failure in one service is isolated and typically does not cascade to the entire system. Technology Stack Homogeneous. Constrained by the technologies chosen at the outset. Heterogeneous (Polyglot). Each service can use the most appropriate language, framework, and database."},{"location":"Database%20per%20Service%20pattern/","title":"Database per Service pattern","text":"<p>The Database per Service pattern is a core tenet of microservices architecture which dictates that each microservice must own and manage its own private database (or a private schema within a shared database server). Crucially, this data store is considered part of the service's internal implementation and must not be directly accessed by any other service. All communication and data exchange must occur exclusively through the service's public API.</p> <p>Pros</p> <ol> <li> <p>Loose Coupling</p> </li> <li> <p>Technology Freedom (Polyglot Persistence)</p> </li> <li> <p>Independent Scalability and Fault Isolation</p> </li> </ol> <p>Cons:</p> <ol> <li> <p>Distributed Transactions: Implementing business transactions that span multiple services becomes highly complex. Traditional ACID (Atomicity, Consistency, Isolation, Durability) transactions, which rely on two-phase commits, are often not feasible or supported by modern NoSQL databases and are generally avoided in microservices due to their negative impact on availability and performance, as described by the CAP theorem.</p> </li> <li> <p>Cross-Service Queries: Performing queries that require joining data from multiple, now-separate databases is challenging. Such operations cannot be handled by a simple SQL join and require more complex patterns.</p> </li> <li> <p>Operational Complexity: The need to deploy, manage, monitor, and back up a multitude of different database technologies increases the operational burden on the organization.</p> </li> </ol> <p>To address the challenge of distributed transactions, the [[SAGA pattern]] is the most common solution.</p>"},{"location":"Domain%20Driven%20Design/","title":"Domain Driven Design","text":"<p>when a new microservice is being created. the most crucial task is to define its boundaries. A microservices value is directly tied to its autonomy and single minded focus. </p> <p>Poorly defined microservices lead to systemic problems, like overly chatty, data consistency issues, distributed monoliths.</p>"},{"location":"Domain%20Driven%20Design/#why-ddd-is-essential","title":"why DDD is essential ?","text":"<p>DDD is a software design methodology that advocates for modeling software based on the underlying business domain. Its core premise is that complex business logic should be at the heart of the application, and the software's structure should reflect the real-world processes and rules of the business. </p> <p>This philosophy aligns perfectly with the goals of microservices architecture. Microservices are intended to be organized around business capabilities, ensuring they have high functional cohesion (doing one thing well) and loose coupling (minimal dependency on other services).</p> <p>DDD provides the tools to achieve this. It helps decompose a large, complex system into self-contained, understandable units by systematically analyzing business requirements.</p>"},{"location":"Domain%20Driven%20Design/#strategic-ddd","title":"Strategic DDD","text":"<p>Focuses on high-level, large-scale, structure of the system. It is a collaborative process that involves both software developers and domain experts (such as business analysts and product managers) to map out the problem space before designing the solution space.</p> <ol> <li>Ubiquitous Language: A cornerstone of DDD is the development of a Ubiquitous Language a common, rigorous, and unambiguous vocabulary shared by all stakeholders.</li> <li>Bounded Context: The central pattern in Strategic DDD is the Bounded Context. A Bounded Context is a conceptual boundary within which a specific domain model and its Ubiquitous Language are valid, consistent, and unambiguous. DDD recognizes that attempting to create a single, unified model for an entire large-scale system is not feasible or cost-effective, as different parts of a business often use the same terms to mean subtly different things.</li> <li>In a microservices architecture, each Bounded Context is a prime candidate to become a microservice. This alignment ensures that each service owns a specific, well-defined part of the business domain, operates autonomously, and maintains its own internal consistency. Defining these boundaries is the most critical step; misdefined boundaries are a primary cause of tight coupling and architectural complexity.</li> <li>Context Maps: To manage the interactions between these boundaries, DDD uses Context Maps. These are diagrams that visualize the relationships between different Bounded Contexts, clarifying how the corresponding microservices will integrate.</li> </ol>"},{"location":"Domain%20Driven%20Design/#tactical-ddd","title":"Tactical DDD","text":"<p>Tactical DDD provides a set of design patterns for building the rich domain model inside a single Bounded Context. These patterns help structure the business logic in a way that is clean, maintainable, and decoupled from infrastructure concerns.</p> <ol> <li>Entities: These are objects defined not by their attributes, but by their thread of continuity and a distinct, persistent identity.</li> <li>Value Objects: These are immutable objects defined by their attributes, lacking a conceptual identity.</li> <li>Aggregates: An Aggregate is a cluster of associated objects (Entities and Value Objects) that are treated as a single unit for the purpose of data changes. Each Aggregate has a root entity, known as the Aggregate Root, which is the sole entry point for any command that modifies the Aggregate. This structure is crucial for enforcing business rules and invariants, as the Aggregate Root ensures that the entire cluster of objects remains in a consistent state after any operation.</li> </ol> <p>Aggregates are particularly important for microservice design. They represent a natural consistency boundary, meaning a single transaction should ideally not span multiple Aggregates. This concept maps directly to microservices, where Aggregates are often excellent candidates for the resources exposed by a service's API.</p>"},{"location":"Domain%20Driven%20Design/#data-consistency-issue","title":"Data Consistency Issue","text":"<p>Once a microservice's boundaries have been defined using Domain-Driven Design, the next logical step is to determine its data management strategy. The architectural choice to isolate services necessitates a corresponding isolation of their data. This leads to the adoption of the [[Database per Service pattern]], a foundational element of microservice design. While this pattern provides significant benefits in terms of autonomy and loose coupling, it introduces the profound challenge of maintaining data consistency and integrity in a distributed environment.</p>"},{"location":"Event%20Sourcing/","title":"Event Sourcing","text":"<p>it is a pattern for data management int distributed systems. </p> <p>Instead of storing only the current state of an entity, Event Sourcing stores the full history of changes as a sequence of immutable events. The current state of an entity is reconstructed by replaying these events in order.</p>"},{"location":"Event%20Sourcing/#will-write-more-about-this-later","title":"WILL WRITE MORE ABOUT THIS LATER","text":""},{"location":"Inter-Service%20Communication/","title":"Inter Service Communication","text":"<p>The way microservices communicate is a defining characteristic of the [[Architectural choice|architecture]], directly influencing performance, coupling, and resilience. A well-designed system often employs a mix of communication styles, choosing the right protocol for each specific interaction.</p> <p>there are mainly 2 different types of communication 1. Synchronous Communication: Direct Request/Response     1. REST over HTTP (Most used) : simple, less complexity, json, stateless, wide support, readable, bad performance.      2. gRPC : more complex, [[protobuf]], binary format (not readable), great performace  2. Asynchronous Communication: Event-Driven Architecture     1. Message Queues (RabbitMQ, Amazon SQS): This model is typically used for command-based communication, where a message represents a task to be performed. A producer sends a message to a specific queue, and a single consumer from a group of workers retrieves and processes that message. This pattern is excellent for distributing workloads, ensuring reliable task processing, and implementing load leveling to smooth out traffic spikes.     2. Event Streams / Publish - Suscribe (Apache kafka, Amazon SNS) : This model is the foundation of event-driven architectures and is used for broadcasting information. A producer service publishes an event\u2014a record of something that has happened\u2014to a topic. Any number of consumer services can subscribe to that topic and will receive a copy of the event to react to it independently. This pattern is ideal for notifying multiple parts of a system about a state change, such as an OrderPlaced or UserRegistered event. </p> <p>Asynchronous communication significantly improves system resilience and scalability. If a consumer service is temporarily unavailable, messages can be buffered in the broker and processed once the service recovers. This prevents failures from cascading. Furthermore, producers and consumers can be scaled independently based on their respective loads, and the non-blocking nature of the communication enhances the perceived responsiveness of the system. The main challenges are the added operational complexity of managing a message broker and the cognitive shift required to design and debug systems based on eventual consistency.</p> <p>A mature microservice ecosystem is rarely built on a single communication style. Instead, it is a hybrid, where the choice of protocol is made on a link-by-link basis, tailored to the specific requirements of each interaction.</p> <p>Table 1: Synchronous vs. Asynchronous Communication Trade-offs</p> Dimension Synchronous Communication Asynchronous Communication Coupling High temporal coupling. Client and server must be available simultaneously. Low coupling. Producer and consumer operate independently and do not need to be available at the same time. Latency Perceived latency is higher for the client, as it must block and wait for a response. Perceived latency is lower for the client (producer), as it does not wait for the operation to complete. Fault Tolerance Lower. A failure in the server service directly impacts the client service. Can lead to cascading failures. Higher. A message broker can buffer messages if a consumer is down, allowing it to recover and process them later. Scalability More limited. Scalability is constrained by the ability of services to handle concurrent requests in real-time. Higher. Producers and consumers can be scaled independently based on message volume and processing time. Implementation Complexity Simpler to reason about and implement, as it follows a familiar request-response flow. More complex. Requires managing a message broker, handling eventual consistency, and debugging distributed workflows. <p>Table 2: Comparison of REST, gRPC, and Message Queues</p> Feature REST over HTTP gRPC Message Queues / Event Streams Communication Style Synchronous Synchronous Asynchronous Performance Moderate. Text-based (JSON) and HTTP/1.1 overhead. High. Binary format (Protobuf) and HTTP/2 transport. Varies. High throughput but introduces broker latency. Decouples processing time from request time. Data Format JSON (human-readable) Protocol Buffers (binary, not human-readable). Any format (JSON, Avro, Protobuf), managed by producer/consumer. Contract Enforcement Loose. Often relies on documentation (e.g., OpenAPI/Swagger). Strict. Contract-first via .proto files, enabling code generation and type safety. Varies. Can be strict with a schema registry (e.g., for Avro/Protobuf) or loose. Coupling Loosely coupled at the implementation level, but tightly coupled temporally. Tightly coupled at the contract level (client/server need .proto file) and temporally. Loosely coupled. Producer and consumer are fully decoupled. Developer Experience Simple and familiar. Excellent tooling and browser support.46 Steeper learning curve. Requires specialized tooling for debugging. Limited browser support. Complex. Requires understanding of messaging concepts and managing a broker. Debugging is challenging. Typical Use Case Public-facing APIs, simple request-response interactions. High-performance internal service-to-service communication, streaming applications. Background jobs, event notification, decoupling services, data streaming pipelines."},{"location":"SAGA%20pattern/","title":"SAGA pattern","text":"<p>To address the challenge of distributed transactions, the Saga pattern is the most common solution for managing data consistency across services. A saga is a sequence of local transactions that are coordinated to execute a complete business process. Each step in the saga is a local, atomic transaction within a single service. Upon successful completion, it triggers the next step in the sequence, typically by publishing an event or sending a command.</p> <p>The key to the Saga pattern's atomicity is its handling of failures. If any local transaction in the sequence fails, the saga executes a series of compensating transactions to undo the work of the preceding, successfully completed steps. For example, if a CreateOrder saga involves ProcessPayment, UpdateInventory, and CreateShipment, and the CreateShipment step fails, compensating transactions for RefundPayment and RestoreInventory would be executed. This approach does not provide the immediate consistency of an ACID transaction but instead achieves eventual consistency, where the system converges to a consistent state over time.</p> <p>There are two primary approaches to implementing sagas:</p> <ol> <li> <p>Choreography: This is a decentralized approach where services communicate directly with each other by publishing and subscribing to events. There is no central coordinator.</p> </li> <li> <p>Orchestration: This is a centralized approach where a dedicated orchestrator service is responsible for managing the entire saga. The orchestrator sends commands to each participant service, waits for a reply, and decides the next step in the workflow. If a step fails, the orchestrator is responsible for triggering the necessary compensating transactions. This approach is easier to monitor and manage for complex workflows but introduces the orchestrator as a potential single point of failure and creates a degree of coupling between the participant services and the orchestrator.</p> </li> </ol>"},{"location":"When%20to%20create%20or%20extend%20a%20service/","title":"When to create or extend a service","text":"<p>There is no universal answer. It must be based on care evaluation of architectural principles and long term consequences. </p>"},{"location":"When%20to%20create%20or%20extend%20a%20service/#reasons-to-create-a-new-service","title":"Reasons to create a new service","text":"<ol> <li>Divergent Responsibilities and the Single Responsibility Principle</li> <li>Independent Scaling Requirements</li> <li>Different Technology Stack</li> <li>Team Ownership and Autonomy</li> </ol>"},{"location":"When%20to%20create%20or%20extend%20a%20service/#reasons-to-extend-a-new-service","title":"Reasons to extend a new service","text":"<ol> <li>High Cohesion and Domain Proximity</li> <li>Avoiding Premature Decomposition : every new service that gets introduces operational overhead, including deployment pipelines, monitoring, security configurations, and network communication latency.</li> </ol> <p>This decision-making process is fundamentally about preserving the [[Architectural choice|architectural integrity]] of the system by protecting the boundaries of each service.</p> <p>The first step when created a new microservice starts with [[Domain Driven Design]].</p>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/","title":"Best Practices for Protobuf","text":"<p>The long-term success of a system built on [[ProtoBuf]] depends heavily on the quality and maintainability of its .proto files. Adhering to best practices is not merely a matter of style but a crucial element of future-proofing the API.</p>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#file-organization","title":"File Organization:","text":"<p>Logically group related messages and services within the same .proto file. For message types that are expected to be widely used across many different services or projects (e.g., a common Money or Timestamp message), it is best to place them in their own separate file with no other dependencies. This promotes reusability and minimizes dependency bloat.</p>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#style-guide","title":"Style Guide:","text":"<p>Consistency is key. The official Google Protobuf Style Guide provides a set of conventions that should be followed :</p> <ul> <li>Formatting: Use a standard line length (80 characters) and indentation (2 spaces).</li> <li>Naming: Use TitleCase for message, service, and enum names. Use lower_snake_case for field and oneof names. Use UPPER_SNAKE_CASE for enum values.    </li> <li>Comments: Use //-style comments to document the purpose of messages, fields, and services. A well-commented .proto file is a form of living documentation for the API.</li> </ul>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#unique-requestresponse-messages","title":"Unique Request/Response Messages:","text":"<p>A critical best practice for RPC service design is to always define unique message types for each method's request and response, even if a method initially takes no parameters or returns nothing. Instead of using a shared or empty type like google.protobuf.Empty, define a specific message (e.g., CreateUserRequest, CreateUserResponse). This practice future-proofs the API. If, in the future, a parameter needs to be added to the request or a new field needs to be returned in the response, it can be done by adding a field to the existing message\u2014a non-breaking change. If google.protobuf.Empty were used, adding any parameter would require changing the method signature, which is a breaking change. This discipline of creating purpose-built messages ensures the API remains extensible for its entire lifetime.</p> Schema Change Backward Compatible? Forward Compatible? Notes Add a new optional or repeated field Yes Yes Safest and most common change. New code uses default values for old data; old code ignores the new field. Remove a field Yes No (Potentially) Old code may fail if it relies on the field. The field number and name should be marked as reserved. Rename a field Yes (Wire Format) Yes (Wire Format) Breaking change for source code. Requires coordinated code updates across all clients and servers. Change a field's number No No Never do this. This is a catastrophic breaking change that leads to data corruption. Change type (int32 to int64) Yes Yes Compatible numeric types can be changed. Change type (int32 to string) No No Incompatible wire formats will cause deserialization errors. Change from singular to repeated No No The wire format is incompatible. Add a field to an existing oneof Yes No Old clients will not recognize the new oneof variant, leading to potential data loss or misinterpretation. Remove a field from an existing oneof No Yes New clients may misinterpret old data containing the removed variant, leading to cascading data loss."},{"location":"ProtoBuf/Interface%20Definition%20Language/","title":"Interface Definition Language","text":""},{"location":"ProtoBuf/Interface%20Definition%20Language/#idl","title":"IDL","text":"<p>The fundamental unit of data structure in [[Protobuf]] is the message. A message is a logical record of information, analogous to a class in Java or a struct in C++ or Go.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#syntax","title":"Syntax:","text":"<p>A message is declared using the message keyword, followed by its name and a pair of curly braces enclosing its field definitions.29</p> <p>```Protocol Buffers message SongRequest { \u00a0 string song_name = 1; \u00a0 int64 artist_id = 2; }  </p> <pre><code>\n\n### Naming Conventions:\n\nThe official Protobuf style guide recommends using **TitleCase**  **for message names** and **lower_snake_case for field names**. Following these conventions ensures that the generated code is idiomatic in the target languages, as the compiler often transforms lower_snake_case into the conventional style of the target language (e.g., camelCase in Java).\n\n### Nested Messages:\n\nMessages can be defined within the scope of other messages. This is useful for grouping related data structures and creating a logical namespace.\n\n```Protocol Buffers\n\nmessage SearchResponse {  \n\u00a0 message Result {  \n\u00a0 \u00a0 string url = 1;  \n\u00a0 \u00a0 string title = 2;  \n\u00a0 }  \n\u00a0 repeated Result results = 1;  \n}  \n</code></pre> <p>Here, Result is a nested message type that can only be accessed within the context of SearchResponse (e.g., SearchResponse.Result in generated code).</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#importing","title":"Importing:","text":"<p>Messages can be defined out of the scope of the message, i.e in a different file, this can be later imported into the required message. </p> <p>result.proto ```Protocol Buffers syntax = \"proto3\";</p> <p>package search;</p> <p>// Define Result independently message Result {   string url = 1;   string title = 2; }</p> <pre><code>\n&gt; **search_response.proto**\n``` Protocol Buffers\nsyntax = \"proto3\";\n\npackage search;\n\nimport \"result.proto\"; // Import the external message definition\n\nmessage SearchResponse {\n  repeated Result results = 1; // Use it directly since it shares the same package\n}\n</code></pre>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#field-numbertag","title":"Field Number/Tag","text":""},{"location":"ProtoBuf/Interface%20Definition%20Language/#syntax_1","title":"Syntax:","text":"<p>Every field in a message must be assigned a unique positive integer number.</p> <p>type field_name = 1;</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#function","title":"Function:","text":"<p>These field numbers, not the field names, are what identify the fields in the compact binary wire format. When a message is serialized, the output is a series of key-value pairs where the key is derived from the field number and a wire type. This design is a primary reason for Protobuf's efficiency and compactness, as it avoids sending verbose string keys (like in JSON) over the network.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#uniqueness-and-immutability","title":"Uniqueness and Immutability:","text":"<p>The rules governing field numbers are strict and are the bedrock of Protobuf's schema evolution capabilities:</p> <ul> <li>The number must be unique within that message.</li> <li>Once a field number is used in a production system, it must never be changed.</li> <li>A field number from a deleted field should never be reused for a new field. Instead, the old number should be marked as reserved to prevent future accidents.</li> </ul> <p>Violating these rules can lead to severe data corruption, as a parser using a newer schema could completely misinterpret data written by an older one. The stability of these numbers is the central pillar of Protobuf's compatibility contract. This prioritization of long-term stability over short-term developer convenience necessitates strong governance over .proto files, including strict code review policies and the use of automated tools to prevent accidental changes to field numbers.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#performance-implications","title":"Performance Implications:","text":"<p>The choice of field number has a direct impact on the size of the serialized message. Field numbers in the range 1 through 15 require only one byte to encode the key (tag + wire type). Numbers in the range 16 through 2047 take two bytes. For this reason, the most frequently used or repeated fields should be assigned the lowest available field numbers.</p> <p>Reserved Ranges:</p> <p>The field numbers from 19,000 to 19,999 are reserved for the internal implementation of Protocol Buffers and cannot be used.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#field-cardinality","title":"Field Cardinality","text":"<p>Each field in a message must have a cardinality, which specifies how many times that field can appear.</p> <ul> <li> <p>Singular Fields: The field can appear at most once in a message. The handling of singular fields differs significantly between Protobuf versions and is a key concept to master.</p> </li> <li> <p>optional (Explicit Presence): This is the recommended approach for modern Protobuf (proto3 with the optional keyword, and the default in editions). It means the system can distinguish between a field that was not set and a field that was explicitly set to its default value (e.g., 0 for an integer). The generated code includes a has_...() method to perform this check.</p> </li> <li> <p>Implicit Presence: This was the original default for scalar fields in proto3. A field holding its default value (e.g., 0, false, \"\") is not serialized. This makes it impossible for the receiver to know if the sender intentionally set the value to 0 or simply omitted the field. This ambiguity can lead to bugs and is why explicit presence is now favored.    </p> </li> <li> <p>repeated Fields: The field can be repeated any number of times (including zero). This is used to represent lists, arrays, or sequences of values. The order of the elements is preserved. The style guide recommends using pluralized names for repeated fields (e.g., repeated Song songs).</p> </li> <li> <p>required Fields (proto2 only, Deprecated): This keyword, available only in proto2, specifies that a value for the field must be provided. While seemingly useful for validation, it proved to be extremely problematic for schema evolution. A field marked as required can never be safely made optional or removed without breaking older clients, which will reject messages that are missing the field. For this reason, its use is strongly discouraged, and it was removed entirely from proto3 and editions.</p> </li> </ul>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#services-for-rpc","title":"Services for RPC","text":"<p>In addition to defining data structures, .proto files are the standard way to define service interfaces for RPC frameworks like gRPC.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#syntax_2","title":"Syntax:","text":"<p>A service is defined using the service keyword, containing a list of rpc methods.  </p> <p><code>Protocol Buffers   service SearchService {     rpc Search(SearchRequest) returns (SearchResponse);   }</code> </p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#rpc-methods","title":"RPC Methods:","text":"<p>Each rpc method specifies its name, its request message type, and its response message type. The compiler uses this definition to generate client stubs and server interface skeletons.    </p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#streaming","title":"Streaming:","text":"<p>The stream keyword can be prepended to the request and/or response types to define different communication patterns, which are a core feature of gRPC:  - Unary RPC: rpc Method(Request) returns (Response); (the default)</p> <ul> <li> <p>Server Streaming RPC: rpc Method(Request) returns (stream Response);</p> </li> <li> <p>Client Streaming RPC: rpc Method(stream Request) returns (Response);</p> </li> <li> <p>Bidirectional Streaming RPC: rpc Method(stream Request) returns (stream Response);</p> </li> </ul> <p>**</p>"},{"location":"ProtoBuf/ProtoBuf%20VS%20JSON%20VS%20XML/","title":"ProtoBuf VS JSON VS XML","text":"Feature Protocol Buffers ([[Protobuf]]) JSON (JavaScript Object Notation) XML (eXtensible Markup Language) Format Type Binary Text-based Text-based Performance (Speed) Very fast serialization/deserialization; up to 6x faster than JSON in server-to-server contexts. Slower than Protobuf due to text parsing overhead. Generally the slowest due to high verbosity and complex parsing rules. Performance (Size) Highly compact; significantly smaller payloads than JSON/XML. More compact than XML but larger than Protobuf due to text-based keys and syntax. Most verbose, resulting in the largest file sizes. Schema Requirement Strictly required; defined in a .proto file. Enforces a strong contract. Schema-less by default; schema can be added via external standards like JSON Schema or OpenAPI. Schema is optional (DTD, XSD) but commonly used in enterprise systems. Human Readability Not human-readable; requires tools for inspection. Human-readable and easy to inspect, aiding in debugging. Human-readable, though verbosity can make it complex to parse visually. Data Type Support Rich set of scalar types, including various integer sizes and encodings; supports enums and complex nested messages. Limited data types (string, number, boolean, array, object, null). Supports a wide range of data types through schema definitions (XSD). Compatibility Excellent backward and forward compatibility features built into the protocol design. No built-in compatibility features; relies on application-level conventions. Schema evolution is possible but can be complex to manage. Primary Use Cases High-performance internal microservices (gRPC), IoT, data storage, and performance-critical systems. Public web APIs, client-side web applications, configuration files, and scenarios valuing simplicity and readability. Enterprise systems, legacy applications, complex document formats, and standards like SOAP."},{"location":"ProtoBuf/ProtoBuf%20Workflow/","title":"ProtoBuf Workflow","text":"<p>[[ProtoBuf]] workflow fundamentally shifts development from a \"code-first\" to a \"contract-first\" paradigm, a change with significant implications for team collaboration and system architecture.</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/#proto-file","title":".proto file","text":"<p>The entire Protobuf workflow originates from a single artifact: the .proto file. This plain text file serves as the definitive contract, or schema, for the data being exchanged. It contains language-agnostic definitions of data structures, known as messages, and optionally, the interfaces for remote services, known as services. This file is the central point of agreement between different components of a distributed system, such as a client and a server. It is managed as a source code asset, version-controlled, and serves as the single source of truth for the API contract.</p> <p>The basic structure of a .proto file includes several key components: - Syntax/Edition Declaration: The first non-comment line must declare the version of the Protobuf language being used, for example, syntax = \"proto3\"; or edition = \"2023\";. - Package Declaration: A package declaration (e.g., package tutorial;) helps prevent naming conflicts between different projects and provides a namespace. - Options: Language-specific option directives can be used to control how code is generated. For instance, option java_package = \"com.example.tutorial\"; specifies the Java package for the generated classes. - Message Definitions: The core of the file, where message blocks define the structure of the data, including field names, types, and unique numbers.</p> <p>By centralizing the API definition in this way, the .proto file becomes the focal point for design reviews and collaboration. Teams can agree on the contract before writing any implementation code, which decouples the interface from the implementation and facilitates parallel development. This \"contract-first\" approach significantly reduces integration errors and creates a system that is inherently self-documenting, with the .proto file acting as formal, machine-readable documentation.</p>"},{"location":"ProtoBuf/ProtoBuf/","title":"ProtoBuf","text":"<p>Originated within Google as an internal project. The primary motivation was to create a more efficient and robust mechanism for communication between Google's vast number of internal services. Full form is \"Protocol Buffers\".</p> <p>XML was a common choice for data interchange but its performance characteristics were inadequate for scale and latency requirement of Google's infrastructure. JSON gained popularity for its simplicity and direct mapping to JavaScript objects, but as a text-based format, it still carries significant size and parsing overhead compared to optimized binary formats.</p> <p>All data structure in protobuf are called messages. </p>"},{"location":"ProtoBuf/ProtoBuf/#core-philosophy","title":"Core Philosophy","text":"<p>The design of Protocol Buffer is built upon a distinct philosophy that prioritizes machine to machine efficiency and contract based design over given readability and ad hoc flexibility. This philosophy is embodied in three core principles.</p> <ol> <li> <p>Schema-Driven :  messages must be explicitly defined in special files with a dot protobuff extension These files are written using [[Interface Definition Language]] that describes the fields of a message their data type and their unique identifying numbers. This schema first approach enforces a strict contract between communicating parties ensuring type safety and data consistency. This contract becomes the single source of truth for the data structure. More details are present in [[ProtoBuf Workflow]].</p> </li> <li> <p>language-agnostic and platform-neutral : The .proto file serves as a universal, language-independent contract. From this single definition, the [[Protoc Compiler|Protobuf compiler]], protoc, generates native source code for a wide array of programming languages, including C++, Java, Python, Go, C#, Dart, and Ruby. The generated code provides a strongly-typed, easy-to-use API for creating, manipulating, serializing, and deserializing the defined messages. This allows services written in different languages to communicate seamlessly, as long as they are compiled from the same .proto definition.</p> </li> <li> <p>performance-oriented : Design is optimized for speed and compactness. The format is a dense binary representation which is significantly smaller than the equivalent text based format like jason or XML Instead of using human readable fields in the payload protobuff uses unique integer tags to identify fields further reducing size It employs efficient encoding techniques such as variable length integers to represent numbers compactly the combination of these technique results in smaller payload which consume less of everything and results in faster [[Serializing and Deserializing in ProtoBuf]].</p> </li> </ol>"},{"location":"ProtoBuf/ProtoBuf/#evolution-of-protobuf","title":"Evolution of Protobuf","text":"Feature proto2 Behavior proto3 Behavior edition = \"2023\" Behavior Syntax Declaration syntax = \"proto2\"; (or omitted) syntax = \"proto3\"; edition = \"2023\"; Field Cardinality required, optional, repeated Singular (implicitly optional), repeated Singular (explicit presence by default), repeated required Keyword Supported, but strongly discouraged for new designs. Removed. Use of required is a compile-time error. Not supported. Migrated required fields use a special feature set. Field Presence Explicit presence for optional fields. A has_ method is generated to check if a field was set. Implicit presence for scalar fields by default (cannot distinguish unset from default value). optional keyword was later added to enable explicit presence. Explicit presence is the default for all singular fields, providing clear tracking of whether a field is set. Default Values Custom default values can be specified using [default = value]. Custom default values are not supported. Fields default to a type-specific zero value (0, empty string, false). Explicit default values are reintroduced. Enums The first enum value can be non-zero. The first enum value must be zero, which serves as the default. The first enum value must be zero. Extensions Natively supported as a core feature. Not supported (though Any provides an alternative). Reintroduced as a feature."},{"location":"ProtoBuf/Protoc%20Compiler/","title":"Protoc Compiler","text":"<p>Once the .proto schema is defined, the next step is to use the Protocol Buffer compiler, protoc. This is a command-line tool that parses the .proto file and generates source code in a specified target programming language. This generated code provides the necessary classes, data structures, and methods to work with the defined messages in a native, idiomatic way for that language.</p> <p>The protoc compiler is designed with a highly extensible plugin architecture. The core protoc binary is responsible for parsing the .proto file into an abstract syntax tree. It then passes this representation to a language-specific plugin, which is responsible for the actual code generation.</p> <p>A typical invocation of the compiler from the command line looks like this:</p> <pre><code>protoc -I=./protos --java_out=./src/main/java./protos/my_message.proto\n</code></pre> <p>The key flags are:</p> <ul> <li> <p>-I or --proto_path: Specifies the directory in which to search for .proto files and their imports. Multiple paths can be provided.</p> </li> <li> <p>--_out: Specifies the output directory for the generated code in the target language (e.g., --java_out, --go_out, --python_out). <p>This generates a set of source files that provide rich, lanuage-specific APIs for the message defined in .proto file. The generated code typically includes:</p> <ol> <li>Message Classes/Structs: A class or struct for each message definition, with properties or fields corresponding to those in the .proto file.</li> <li>Accessors: Getters and setters for each field, allowing for easy and type-safe manipulation of the data.</li> <li>Builders/Factories: Fluent builder patterns or factory methods for constructing message instances in a readable and convenient way (e.g., Person.newBuilder().setName(\"John\").setId(123).build();).</li> <li>Presence Methods: For fields with explicit presence (optional fields in proto2 or proto3, or all singular fields in editions), methods are generated to check if a field has been set (e.g., has_name()).</li> <li>Serialization Methods: Functions to serialize the message object into a compact binary byte array (e.g., SerializeToString(), toByteArray(), writeTo(OutputStream)).</li> <li>Deserialization Methods: Static methods or functions to parse a binary byte array back into a message object (e.g., ParseFromString(), parseFrom(InputStream)).</li> </ol>"},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/","title":"Serializing and Deserializing in ProtoBuf","text":""},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/#serializing","title":"Serializing","text":"<p>The process of serializing a message typically involves three steps:</p> <ol> <li> <p>Instantiation: Create an instance of the generated message class, often using a builder pattern for clarity and to ensure the object is immutable once created.</p> </li> <li> <p>Population: Use the generated setter methods to populate the fields of the message with application data.</p> </li> <li> <p>Serialization: Call one of the serialization methods (e.g., SerializeToString() or toByteArray()) on the final message object. This executes the highly optimized serialization logic, converting the in-memory object into a compact byte array ready for transmission or storage.</p> </li> </ol> <p>The resulting binary data is not self-describing. It is a stream of key-value pairs, where the \"key\" is a combination of the field number and a wire type identifier, and the \"value\" is the encoded data payload. The field names and precise data types are not included in the payload. This is a fundamental reason for [[Serializing and Deserializing in ProtoBuf|Protobuf's efficiency]], but it also means that the exact same schema definition (or the code generated from it) is required to correctly interpret the data on the receiving end.</p>"},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/#deserializing","title":"Deserializing","text":"<p>The deserialization process is the mirror image of serialization:</p> <ol> <li> <p>Receive Data: Obtain the serialized byte array, for example, from a network socket or a file.</p> </li> <li> <p>Parsing: Call a static parsing method on the corresponding generated message class, passing the byte array as an argument (e.g., Person.parseFrom(data)).</p> </li> <li> <p>Usage: The parsing method returns a fully populated, read-only instance of the message object. The application can then use the generated getter methods to access the data.</p> </li> </ol>"}]}