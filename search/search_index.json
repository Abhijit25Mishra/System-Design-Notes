{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"System Design Notes","text":"<p>Welcome to System Design Notes, a curated collection of computer science and distributed systems.</p>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Distributed Systems</li> <li>ProtoBuf</li> </ul>"},{"location":"Distributed%20Systems/","title":"Distributed Systems","text":"<p>Collection of architecture patterns, domain-driven design notes, event sourcing, communication patterns, and practical guidance for building distributed systems.</p> <p>Key topics in this folder:</p> <ul> <li><code>Architectural choice</code> \u2014 monolithic vs microservices trade-offs and decision guidance.</li> <li><code>Domain Driven Design</code> \u2014 strategic and tactical DDD patterns for defining service boundaries.</li> <li><code>Database per Service pattern</code> \u2014 data ownership, pros/cons, and operational considerations.</li> <li><code>SAGA pattern</code> \u2014 handling distributed transactions and eventual consistency.</li> <li><code>Event Sourcing</code> \u2014 storing events as the source of truth and reconstructing state.</li> <li><code>Inter-Service Communication</code> \u2014 synchronous vs asynchronous styles, gRPC, REST, message queues, and trade-offs.</li> <li><code>When to create or extend a service</code> \u2014 guidelines for service decomposition.</li> </ul> <p>Use these pages to help design, model, and operate robust distributed systems.</p>"},{"location":"Distributed%20Systems/Domain%20Driven%20Design/","title":"Domain Driven Design","text":"<p>When a new microservice is being created, the most crucial task is to define its boundaries. A microservice's value is directly tied to its autonomy and single-minded focus. </p> <p>Poorly defined microservices lead to systemic problems, like overly chatty communication, data consistency issues, and distributed monoliths.</p>"},{"location":"Distributed%20Systems/Domain%20Driven%20Design/#why-is-ddd-essential","title":"Why is DDD Essential?","text":"<p>DDD is a software design methodology that advocates for modeling software based on the underlying business domain. Its core premise is that complex business logic should be at the heart of the application, and the software's structure should reflect the real-world processes and rules of the business. </p> <p>This philosophy aligns perfectly with the goals of microservices architecture. Microservices are intended to be organized around business capabilities, ensuring they have high functional cohesion (doing one thing well) and loose coupling (minimal dependency on other services).</p> <p>DDD provides the tools to achieve this. It helps decompose a large, complex system into self-contained, understandable units by systematically analyzing business requirements.</p>"},{"location":"Distributed%20Systems/Domain%20Driven%20Design/#strategic-ddd","title":"Strategic DDD","text":"<p>Strategic DDD focuses on the high-level, large-scale structure of the system. It is a collaborative process that involves both software developers and domain experts (such as business analysts and product managers) to map out the problem space before designing the solution space.</p> <ol> <li> <p>Ubiquitous Language: A cornerstone of DDD is the development of a Ubiquitous Language - a common, rigorous, and unambiguous vocabulary shared by all stakeholders.</p> </li> <li> <p>Bounded Context: The central pattern in Strategic DDD is the Bounded Context. A Bounded Context is a conceptual boundary within which a specific domain model and its Ubiquitous Language are valid, consistent, and unambiguous. DDD recognizes that attempting to create a single, unified model for an entire large-scale system is not feasible or cost-effective, as different parts of a business often use the same terms to mean subtly different things.</p> </li> <li> <p>Microservices Alignment: In a microservices architecture, each Bounded Context is a prime candidate to become a microservice. This alignment ensures that each service owns a specific, well-defined part of the business domain, operates autonomously, and maintains its own internal consistency. Defining these boundaries is the most critical step; misdefined boundaries are a primary cause of tight coupling and architectural complexity.</p> </li> <li> <p>Context Maps: To manage the interactions between these boundaries, DDD uses Context Maps. These are diagrams that visualize the relationships between different Bounded Contexts, clarifying how the corresponding microservices will integrate.</p> </li> </ol>"},{"location":"Distributed%20Systems/Domain%20Driven%20Design/#tactical-ddd","title":"Tactical DDD","text":"<p>Tactical DDD provides a set of design patterns for building the rich domain model inside a single Bounded Context. These patterns help structure the business logic in a way that is clean, maintainable, and decoupled from infrastructure concerns.</p> <ol> <li> <p>Entities: Objects defined not by their attributes, but by their thread of continuity and a distinct, persistent identity.</p> </li> <li> <p>Value Objects: Immutable objects defined by their attributes, lacking a conceptual identity.</p> </li> <li> <p>Aggregates: A cluster of associated objects (Entities and Value Objects) that are treated as a single unit for the purpose of data changes. Each Aggregate has a root entity, known as the Aggregate Root, which is the sole entry point for any command that modifies the Aggregate. This structure is crucial for enforcing business rules and invariants, as the Aggregate Root ensures that the entire cluster of objects remains in a consistent state after any operation.Aggregates are particularly important for microservice design. They represent a natural consistency boundary, meaning a single transaction should ideally not span multiple Aggregates. This concept maps directly to microservices, where Aggregates are often excellent candidates for the resources exposed by a service's API.</p> </li> </ol>"},{"location":"Distributed%20Systems/Domain%20Driven%20Design/#data-consistency-issue","title":"Data Consistency Issue","text":"<p>Once a microservice's boundaries have been defined using Domain-Driven Design, the next logical step is to determine its data management strategy. The architectural choice to isolate services necessitates a corresponding isolation of their data. This leads to the adoption of the [[Database per Service pattern]], a foundational element of microservice design. While this pattern provides significant benefits in terms of autonomy and loose coupling, it introduces the profound challenge of maintaining data consistency and integrity in a distributed environment.</p>"},{"location":"Distributed%20Systems/Event%20Sourcing/","title":"Event Sourcing","text":"<p>Event Sourcing is a pattern for data management in distributed systems. </p> <p>Instead of storing only the current state of an entity, Event Sourcing stores the full history of changes as a sequence of immutable events. The current state of an entity is reconstructed by replaying these events in order.</p>"},{"location":"Distributed%20Systems/Event%20Sourcing/#todo","title":"TODO","text":"<p>Will write more about this later</p>"},{"location":"Distributed%20Systems/Inter-Service%20Communication/","title":"Inter-Service Communication","text":"<p>The way microservices communicate is a defining characteristic of the [[Architectural choice|architecture]], directly influencing performance, coupling, and resilience. A well-designed system often employs a mix of communication styles, choosing the right protocol for each specific interaction.</p> <p>There are mainly two different types of communication:</p> <ol> <li> <p>Synchronous Communication: Direct Request/Response</p> <ul> <li>REST over HTTP (Most used): Simple, less complexity, JSON, stateless, wide support, readable, lower performance</li> <li>gRPC: More complex, [[protobuf]], binary format (not readable), great performance</li> </ul> </li> <li> <p>Asynchronous Communication: Event-Driven Architecture</p> <ul> <li>Message Queues (RabbitMQ, Amazon SQS): This model is typically used for command-based communication, where a message represents a task to be performed. A producer sends a message to a specific queue, and a single consumer from a group of workers retrieves and processes that message. This pattern is excellent for distributing workloads, ensuring reliable task processing, and implementing load leveling to smooth out traffic spikes.</li> <li>Event Streams / Publish-Subscribe (Apache Kafka, Amazon SNS): This model is the foundation of event-driven architectures and is used for broadcasting information. A producer service publishes an event\u2014a record of something that has happened\u2014to a topic. Any number of consumer services can subscribe to that topic and will receive a copy of the event to react to it independently. This pattern is ideal for notifying multiple parts of a system about a state change, such as an <code>OrderPlaced</code> or <code>UserRegistered</code> event.</li> </ul> </li> </ol> <p>Asynchronous communication significantly improves system resilience and scalability. If a consumer service is temporarily unavailable, messages can be buffered in the broker and processed once the service recovers. This prevents failures from cascading. Furthermore, producers and consumers can be scaled independently based on their respective loads, and the non-blocking nature of the communication enhances the perceived responsiveness of the system. The main challenges are the added operational complexity of managing a message broker and the cognitive shift required to design and debug systems based on eventual consistency.</p> <p>A mature microservice ecosystem is rarely built on a single communication style. Instead, it is a hybrid, where the choice of protocol is made on a link-by-link basis, tailored to the specific requirements of each interaction.</p> <p>Table 1: Synchronous vs. Asynchronous Communication Trade-offs</p> Dimension Synchronous Communication Asynchronous Communication Coupling High temporal coupling. Client and server must be available simultaneously. Low coupling. Producer and consumer operate independently and do not need to be available at the same time. Latency Perceived latency is higher for the client, as it must block and wait for a response. Perceived latency is lower for the client (producer), as it does not wait for the operation to complete. Fault Tolerance Lower. A failure in the server service directly impacts the client service. Can lead to cascading failures. Higher. A message broker can buffer messages if a consumer is down, allowing it to recover and process them later. Scalability More limited. Scalability is constrained by the ability of services to handle concurrent requests in real-time. Higher. Producers and consumers can be scaled independently based on message volume and processing time. Implementation Complexity Simpler to reason about and implement, as it follows a familiar request-response flow. More complex. Requires managing a message broker, handling eventual consistency, and debugging distributed workflows. <p>Table 2: Comparison of REST, gRPC, and Message Queues</p> Feature REST over HTTP gRPC Message Queues / Event Streams Communication Style Synchronous Synchronous Asynchronous Performance Moderate. Text-based (JSON) and HTTP/1.1 overhead. High. Binary format (Protobuf) and HTTP/2 transport. Varies. High throughput but introduces broker latency. Decouples processing time from request time. Data Format JSON (human-readable) Protocol Buffers (binary, not human-readable). Any format (JSON, Avro, Protobuf), managed by producer/consumer. Contract Enforcement Loose. Often relies on documentation (e.g., OpenAPI/Swagger). Strict. Contract-first via .proto files, enabling code generation and type safety. Varies. Can be strict with a schema registry (e.g., for Avro/Protobuf) or loose. Coupling Loosely coupled at the implementation level, but tightly coupled temporally. Tightly coupled at the contract level (client/server need .proto file) and temporally. Loosely coupled. Producer and consumer are fully decoupled. Developer Experience Simple and familiar. Excellent tooling and browser support.46 Steeper learning curve. Requires specialized tooling for debugging. Limited browser support. Complex. Requires understanding of messaging concepts and managing a broker. Debugging is challenging. Typical Use Case Public-facing APIs, simple request-response interactions. High-performance internal service-to-service communication, streaming applications. Background jobs, event notification, decoupling services, data streaming pipelines."},{"location":"Distributed%20Systems/When%20to%20create%20or%20extend%20a%20service/","title":"When to Create or Extend a Service","text":"<p>There is no universal answer. The decision must be based on careful evaluation of architectural principles and long-term consequences. </p>"},{"location":"Distributed%20Systems/When%20to%20create%20or%20extend%20a%20service/#reasons-to-create-a-new-service","title":"Reasons to Create a New Service","text":"<ol> <li>Divergent Responsibilities and the Single Responsibility Principle</li> <li>Service has clearly separate and distinct business functionality</li> <li> <p>Follows single responsibility principle</p> </li> <li> <p>Independent Scaling Requirements</p> </li> <li>Different performance characteristics</li> <li> <p>Unique resource demands</p> </li> <li> <p>Different Technology Stack</p> </li> <li>Requires specific technologies or frameworks</li> <li> <p>Incompatible with current service architecture</p> </li> <li> <p>Team Ownership and Autonomy</p> </li> <li>Clear ownership boundaries</li> <li>Independent development and deployment cycles</li> </ol>"},{"location":"Distributed%20Systems/When%20to%20create%20or%20extend%20a%20service/#reasons-to-extend-an-existing-service","title":"Reasons to Extend an Existing Service","text":"<ol> <li>High Cohesion and Domain Proximity</li> <li>Functionality is closely related to existing service</li> <li> <p>Shares common domain concepts and data models</p> </li> <li> <p>Avoiding Premature Decomposition</p> </li> <li>Every new service introduces operational overhead:<ul> <li>Deployment pipelines</li> <li>Monitoring systems</li> <li>Security configurations</li> <li>Network communication latency</li> </ul> </li> </ol>"},{"location":"Distributed%20Systems/When%20to%20create%20or%20extend%20a%20service/#decision-making-process","title":"Decision Making Process","text":"<p>This decision-making process is fundamentally about preserving the [[Architectural choice|architectural integrity]] of the system by protecting the boundaries of each service.</p> <p>When creating a new microservice, always start with [[Domain Driven Design]] to properly define its boundaries and responsibilities.</p>"},{"location":"Distributed%20Systems/Architectural-Style/","title":"Architectural Styles","text":"<p>This section explores fundamental architectural choices for distributed systems, focusing on the trade-offs and characteristics of different styles.</p>"},{"location":"Distributed%20Systems/Architectural-Style/#key-topics","title":"Key Topics","text":"<ul> <li><code>Architectural choice</code> \u2014 A detailed comparison of Monolithic and Microservices architectures, including their advantages, disadvantages, and suitability for various scenarios.</li> </ul> <p>Use this page to understand the foundational decisions in system architecture.</p>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/","title":"Architectural Choices","text":"<p>This document compares two main architectural patterns: Monolithic and Microservices architectures.</p>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#monolithic-architecture","title":"Monolithic Architecture","text":"<p>A monolithic architecture is characterized by: - Single, unified codebase - All business functions in one application - Interdependent components - Single-unit deployment</p>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#advantages","title":"Advantages","text":"<ol> <li>Team Size Compatibility</li> <li>Ideal for small-scale teams</li> <li> <p>Easier team coordination</p> </li> <li> <p>Development Simplicity</p> </li> <li>Faster development cycles</li> <li> <p>Straightforward deployment process</p> </li> <li> <p>Request Tracing</p> </li> <li>Easier to trace and debug requests</li> <li>Single codebase to search</li> </ol>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#disadvantages","title":"Disadvantages","text":"<p>As the application scales, advantages often become challenges:</p> <ol> <li>Development Speed</li> <li>Slower development cycles</li> <li> <p>Higher risk of unintended side effects</p> </li> <li> <p>Scaling Issues</p> </li> <li>Inefficient resource utilization</li> <li> <p>Must scale entire application</p> </li> <li> <p>Reliability Concerns</p> </li> <li>Single points of failure</li> <li> <p>One error affects entire application</p> </li> <li> <p>Technology Constraints</p> </li> <li>Difficult to adopt new technologies</li> <li>Stack decisions affect entire application</li> </ol>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#microservices-architecture","title":"Microservices Architecture","text":"<p>A microservices architecture consists of: - Multiple independent applications - Services operate independently - Loose coupling between services - Each service handles specific business functions - Communicates through APIs.</p>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#advantages_1","title":"Advantages","text":"<ol> <li>Flexible Scaling</li> <li>Handle distinct scaling requirements</li> <li>Scale services independently</li> <li> <p>Optimize resource usage</p> </li> <li> <p>Technology Freedom</p> </li> <li>Enable technological diversity</li> <li>Choose best tools for each service</li> <li> <p>Easier to adopt new technologies</p> </li> <li> <p>Improved Reliability</p> </li> <li>Better fault isolation</li> <li>Failures don't cascade easily</li> <li>Higher system resilience</li> </ol>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#key-benefits","title":"Key Benefits","text":"<p>Microservices solve many problems present in monolithic architectures. The adoption is often driven by organizational scaling needs rather than just technical requirements. </p> <p>When working with a monolithic codebase, large teams face: - High coordination overhead - Frequent merge conflicts - Bottlenecked release pipeline</p> <p>Microservices help by: - Minimizing cross-team communication overhead - Enabling parallel development - Supporting independent deployment</p> <p>However, data consistency is one area where monolithic architectures often have an advantage, as they can more easily maintain strong consistency across the application.</p>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#service-creation-decision","title":"Service Creation Decision","text":"<p>For guidance on when to create new services or extend existing ones, see [[When to create or extend a service]].</p>"},{"location":"Distributed%20Systems/Architectural-Style/Architectural%20choice/#architecture-comparison","title":"Architecture Comparison","text":"Dimension Monolithic Architecture Microservices Architecture Team Size Ideal for small teams (&lt;10-15 developers) where communication is simple. Suitable for larger organizations with multiple teams that need to work independently. Development Speed High initial velocity due to simplicity. Slower long-term velocity as complexity grows. Slower initial setup due to distributed system complexity. Higher long-term velocity for large teams. Scalability Coarse-grained. The entire application must be scaled, even if only one component is a bottleneck. Fine-grained. Each service can be scaled independently, optimizing resource usage. Deployment Simple. A single artifact is deployed. Complex. Requires mature automation (CI/CD) and container orchestration for many services. Operational Overhead Low. Fewer moving parts to manage, monitor, and secure. High. Requires managing a distributed system, including service discovery, distributed tracing, and complex networking. Data Consistency Strong consistency is easily achieved with a single, transactional database (ACID). Eventual consistency is the default. Achieving strong consistency across services is complex and requires patterns like Sagas. Fault Isolation Low. An error in one module can bring down the entire application. High. Failure in one service is isolated and typically does not cascade to the entire system. Technology Stack Homogeneous. Constrained by the technologies chosen at the outset. Heterogeneous (Polyglot). Each service can use the most appropriate language, framework, and database."},{"location":"Distributed%20Systems/Microservices-Patterns/","title":"Microservices Patterns","text":"<p>This section details common architectural patterns used in microservices to address challenges like data management and distributed transactions.</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/#key-patterns","title":"Key Patterns","text":"<ul> <li><code>Database per Service pattern</code> \u2014 Discusses the principle of each microservice owning its private database, including advantages, challenges, and implications for data consistency.</li> <li><code>SAGA pattern</code> \u2014 Explores how to manage distributed transactions across multiple services to maintain data consistency, covering both choreography and orchestration approaches.</li> </ul> <p>Use these patterns to build robust and scalable microservices architectures.</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/Database%20per%20Service%20pattern/","title":"Database per Service Pattern","text":"<p>The Database per Service pattern is a core tenet of microservices architecture which dictates that each microservice must own and manage its own private database (or a private schema within a shared database server). </p> <p>Crucially, this data store is considered part of the service's internal implementation and must not be directly accessed by any other service. All communication and data exchange must occur exclusively through the service's public API.</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/Database%20per%20Service%20pattern/#advantages","title":"Advantages","text":"<ol> <li> <p>Loose Coupling: Services maintain complete control over their data schema and can evolve independently.</p> </li> <li> <p>Technology Freedom (Polyglot Persistence): Each service can choose the most appropriate database technology for its specific needs.</p> </li> <li> <p>Independent Scalability and Fault Isolation: Services can scale their databases independently based on their specific load patterns and requirements.</p> </li> </ol>"},{"location":"Distributed%20Systems/Microservices-Patterns/Database%20per%20Service%20pattern/#challenges","title":"Challenges","text":"<ol> <li> <p>Distributed Transactions: Implementing business transactions that span multiple services becomes highly complex. Traditional ACID (Atomicity, Consistency, Isolation, Durability) transactions, which rely on two-phase commits, are often not feasible or supported by modern NoSQL databases. They are generally avoided in microservices due to their negative impact on availability and performance, as described by the CAP theorem.</p> </li> <li> <p>Cross-Service Queries: Performing queries that require joining data from multiple, now-separate databases is challenging. Such operations cannot be handled by a simple SQL join and require more complex patterns.</p> </li> <li> <p>Operational Complexity: The need to deploy, manage, monitor, and back up a multitude of different database technologies increases the operational burden on the organization.</p> </li> </ol>"},{"location":"Distributed%20Systems/Microservices-Patterns/Database%20per%20Service%20pattern/#solution-for-distributed-transactions","title":"Solution for Distributed Transactions","text":"<p>To address the challenge of distributed transactions, the [[SAGA pattern]] is the most common solution. This pattern helps maintain data consistency across services through a sequence of local transactions.</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/SAGA%20pattern/","title":"SAGA Pattern","text":"<p>The SAGA pattern is the most common solution for managing data consistency across services in distributed systems. A saga is a sequence of local transactions that are coordinated to execute a complete business process. Each step in the saga is a local, atomic transaction within a single service. Upon successful completion, it triggers the next step in the sequence, typically by publishing an event or sending a command.</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/SAGA%20pattern/#handling-failures","title":"Handling Failures","text":"<p>The key to the SAGA pattern's atomicity is its handling of failures. If any local transaction in the sequence fails, the saga executes a series of compensating transactions to undo the work of the preceding, successfully completed steps. </p> <p>For example, if a <code>CreateOrder</code> saga involves: 1. <code>ProcessPayment</code> 2. <code>UpdateInventory</code> 3. <code>CreateShipment</code></p> <p>And the <code>CreateShipment</code> step fails, compensating transactions for: - <code>RefundPayment</code> - <code>RestoreInventory</code></p> <p>would be executed. This approach does not provide the immediate consistency of an ACID transaction but instead achieves eventual consistency, where the system converges to a consistent state over time.</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/SAGA%20pattern/#implementation-approaches","title":"Implementation Approaches","text":"<p>There are two primary approaches to implementing sagas:</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/SAGA%20pattern/#1-choreography","title":"1. Choreography","text":"<p>This is a decentralized approach where: - Services communicate directly with each other - Communication happens through publishing and subscribing to events - No central coordinator is needed</p>"},{"location":"Distributed%20Systems/Microservices-Patterns/SAGA%20pattern/#2-orchestration","title":"2. Orchestration","text":"<p>This is a centralized approach where: - A dedicated orchestrator service manages the entire saga - The orchestrator sends commands to each participant service - It waits for replies and decides the next steps in the workflow - If a step fails, the orchestrator triggers the necessary compensating transactions</p> <p>The orchestration approach is easier to monitor and manage for complex workflows but has two main drawbacks: - Introduces the orchestrator as a potential single point of failure - Creates a degree of coupling between the participant services and the orchestrator</p>"},{"location":"ProtoBuf/","title":"Protocol Buffers (ProtoBuf)","text":"<p>This section collects notes, best practices, and workflow guidance for Protocol Buffers (ProtoBuf).</p> <p>ProtoBuf is a compact, efficient binary serialization format and IDL used for defining messages and RPC services. The pages in this folder cover:</p> <ul> <li><code>ProtoBuf Workflow</code> \u2014 contract-first development and <code>.proto</code> file structure.</li> <li><code>Protoc Compiler</code> \u2014 how to invoke <code>protoc</code> and useful flags/plugins.</li> <li><code>Interface Definition Language</code> \u2014 message syntax, naming conventions, and examples.</li> <li><code>Serializing and Deserializing in ProtoBuf</code> \u2014 runtime serialization and parsing patterns.</li> <li><code>Best Practices for Protobuf</code> \u2014 style, file organization, and schema evolution guidance.</li> <li><code>ProtoBuf VS JSON VS XML</code> \u2014 comparison of formats and trade-offs.</li> </ul> <p>Use the links above to jump to a specific topic.</p>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/","title":"Best Practices for Protocol Buffers","text":"<p>The long-term success of a system built on [[ProtoBuf]] depends heavily on the quality and maintainability of its <code>.proto</code> files. Adhering to best practices is not merely a matter of style but a crucial element of future-proofing the API.</p>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#file-organization","title":"File Organization","text":"<p>Logically group related messages and services within the same <code>.proto</code> file. For message types that are expected to be widely used across many different services or projects (e.g., a common <code>Money</code> or <code>Timestamp</code> message), it is best to place them in their own separate file with no other dependencies. This promotes reusability and minimizes dependency bloat.</p>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#style-guide","title":"Style Guide","text":"<p>Consistency is key. The official Google Protobuf Style Guide provides a set of conventions that should be followed:</p> <ul> <li>Formatting: Use a standard line length (80 characters) and indentation (2 spaces).</li> <li>Naming: Use <code>TitleCase</code> for message, service, and enum names. Use <code>lower_snake_case</code> for field and oneof names. Use <code>UPPER_SNAKE_CASE</code> for enum values.    </li> <li>Comments: Use <code>//</code>-style comments to document the purpose of messages, fields, and services. A well-commented <code>.proto</code> file is a form of living documentation for the API.</li> </ul>"},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#unique-requestresponse-messages","title":"Unique Request/Response Messages","text":"<p>A critical best practice for RPC service design is to always define unique message types for each method's request and response, even if a method initially takes no parameters or returns nothing. Instead of using a shared or empty type like <code>google.protobuf.Empty</code>, define a specific message (e.g., <code>CreateUserRequest</code>, <code>CreateUserResponse</code>). </p> <p>This practice future-proofs the API. If, in the future, a parameter needs to be added to the request or a new field needs to be returned in the response, it can be done by adding a field to the existing message\u2014a non-breaking change. If <code>google.protobuf.Empty</code> were used, adding any parameter would require changing the method signature, which is a breaking change.</p> <p>This discipline of creating purpose-built messages ensures the API remains extensible for its entire lifetime.</p> Schema Change Backward Compatible? Forward Compatible? Notes Add a new optional or repeated field Yes Yes Safest and most common change. New code uses default values for old data; old code ignores the new field. Remove a field Yes No (Potentially) Old code may fail if it relies on the field. The field number and name should be marked as reserved. Rename a field Yes (Wire Format) Yes (Wire Format) Breaking change for source code. Requires coordinated code updates across all clients and servers. Change a field's number No No Never do this. This is a catastrophic breaking change that leads to data corruption. Change type (int32 to int64) Yes Yes Compatible numeric types can be changed. Change type (int32 to string) No No Incompatible wire formats will cause deserialization errors. Change from singular to repeated No No The wire format is incompatible. Add a field to an existing oneof Yes No Old clients will not recognize the new oneof variant, leading to potential data loss or misinterpretation. Remove a field from an existing oneof No Yes New clients may misinterpret old data containing the removed variant, leading to cascading data loss."},{"location":"ProtoBuf/Best%20Practices%20for%20Protobuf/#protobuf-bestpractise","title":"protobuf #bestpractise","text":""},{"location":"ProtoBuf/Interface%20Definition%20Language/","title":"Interface Definition Language (IDL)","text":"<p>The fundamental unit of data structure in [[Protobuf]] is the message. A message is a logical record of information, analogous to a class in Java or a struct in C++ or Go.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#syntax","title":"Syntax","text":"<p>A message is declared using the <code>message</code> keyword, followed by its name and a pair of curly braces enclosing its field definitions.</p> <pre><code>message SongRequest {\n    string song_name = 1;\n    int64 artist_id = 2;\n}\n</code></pre>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#naming-conventions","title":"Naming Conventions","text":"<p>The official Protobuf style guide recommends using: - <code>TitleCase</code> for message names - <code>lower_snake_case</code> for field names</p> <p>Following these conventions ensures that the generated code is idiomatic in the target languages, as the compiler often transforms <code>lower_snake_case</code> into the conventional style of the target language (e.g., <code>camelCase</code> in Java).### Nested Messages:</p> <p>Messages can be defined within the scope of other messages. This is useful for grouping related data structures and creating a logical namespace.</p> <pre><code>message SearchResponse {  \n\u00a0 message Result {  \n\u00a0 \u00a0 string url = 1;  \n\u00a0 \u00a0 string title = 2;  \n\u00a0 }  \n\u00a0 repeated Result results = 1;  \n}  \n</code></pre> <p>Here, Result is a nested message type that can only be accessed within the context of SearchResponse (e.g., SearchResponse.Result in generated code).</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#importing","title":"Importing:","text":"<p>Messages can be defined out of the scope of the message, i.e in a different file, this can be later imported into the required message. </p> <p>result.proto <pre><code>syntax = \"proto3\";\n\npackage search;\n\n// Define Result independently\nmessage Result {\n  string url = 1;\n  string title = 2;\n}\n</code></pre></p> <p>search_response.proto <pre><code>syntax = \"proto3\";\n\npackage search;\n\nimport \"result.proto\"; // Import the external message definition\n\nmessage SearchResponse {\n  repeated Result results = 1; // Use it directly since it shares the same package\n}\n</code></pre></p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#field-numbertag","title":"Field Number/Tag","text":""},{"location":"ProtoBuf/Interface%20Definition%20Language/#syntax_1","title":"Syntax:","text":"<p>Every field in a message must be assigned a unique positive integer number.</p> <p>type field_name = 1;</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#function","title":"Function:","text":"<p>These field numbers, not the field names, are what identify the fields in the compact binary wire format. When a message is serialized, the output is a series of key-value pairs where the key is derived from the field number and a wire type. This design is a primary reason for Protobuf's efficiency and compactness, as it avoids sending verbose string keys (like in JSON) over the network.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#uniqueness-and-immutability","title":"Uniqueness and Immutability:","text":"<p>The rules governing field numbers are strict and are the bedrock of Protobuf's schema evolution capabilities:</p> <ul> <li>The number must be unique within that message.</li> <li>Once a field number is used in a production system, it must never be changed.</li> <li>A field number from a deleted field should never be reused for a new field. Instead, the old number should be marked as reserved to prevent future accidents.</li> </ul> <p>Violating these rules can lead to severe data corruption, as a parser using a newer schema could completely misinterpret data written by an older one. The stability of these numbers is the central pillar of Protobuf's compatibility contract. This prioritization of long-term stability over short-term developer convenience necessitates strong governance over .proto files, including strict code review policies and the use of automated tools to prevent accidental changes to field numbers.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#performance-implications","title":"Performance Implications:","text":"<p>The choice of field number has a direct impact on the size of the serialized message. Field numbers in the range 1 through 15 require only one byte to encode the key (tag + wire type). Numbers in the range 16 through 2047 take two bytes. For this reason, the most frequently used or repeated fields should be assigned the lowest available field numbers.</p> <p>Reserved Ranges:</p> <p>The field numbers from 19,000 to 19,999 are reserved for the internal implementation of Protocol Buffers and cannot be used.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#field-cardinality","title":"Field Cardinality","text":"<p>Each field in a message must have a cardinality, which specifies how many times that field can appear.</p> <ul> <li> <p>Singular Fields: The field can appear at most once in a message. The handling of singular fields differs significantly between Protobuf versions and is a key concept to master.</p> </li> <li> <p>optional (Explicit Presence): This is the recommended approach for modern Protobuf (proto3 with the optional keyword, and the default in editions). It means the system can distinguish between a field that was not set and a field that was explicitly set to its default value (e.g., 0 for an integer). The generated code includes a has_...() method to perform this check.</p> </li> <li> <p>Implicit Presence: This was the original default for scalar fields in proto3. A field holding its default value (e.g., 0, false, \"\") is not serialized. This makes it impossible for the receiver to know if the sender intentionally set the value to 0 or simply omitted the field. This ambiguity can lead to bugs and is why explicit presence is now favored.    </p> </li> <li> <p>repeated Fields: The field can be repeated any number of times (including zero). This is used to represent lists, arrays, or sequences of values. The order of the elements is preserved. The style guide recommends using pluralized names for repeated fields (e.g., repeated Song songs).</p> </li> <li> <p>required Fields (proto2 only, Deprecated): This keyword, available only in proto2, specifies that a value for the field must be provided. While seemingly useful for validation, it proved to be extremely problematic for schema evolution. A field marked as required can never be safely made optional or removed without breaking older clients, which will reject messages that are missing the field. For this reason, its use is strongly discouraged, and it was removed entirely from proto3 and editions.</p> </li> </ul>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#services-for-rpc","title":"Services for RPC","text":"<p>In addition to defining data structures, .proto files are the standard way to define service interfaces for RPC frameworks like gRPC.</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#syntax_2","title":"Syntax:","text":"<p>A service is defined using the service keyword, containing a list of rpc methods.  </p> <p><code>Protocol   service SearchService {     rpc Search(SearchRequest) returns (SearchResponse);   }</code> </p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#rpc-methods","title":"RPC Methods:","text":"<p>Each rpc method specifies its name, its request message type, and its response message type. The compiler uses this definition to generate client stubs and server interface skeletons.    </p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#streaming","title":"Streaming:","text":"<p>The stream keyword can be prepended to the request and/or response types to define different communication patterns, which are a core feature of gRPC:  - Unary RPC: rpc Method(Request) returns (Response); (the default)</p> <ul> <li> <p>Server Streaming RPC: rpc Method(Request) returns (stream Response);</p> </li> <li> <p>Client Streaming RPC: rpc Method(stream Request) returns (Response);</p> </li> <li> <p>Bidirectional Streaming RPC: rpc Method(stream Request) returns (stream Response);</p> </li> </ul> <p>**</p>"},{"location":"ProtoBuf/Interface%20Definition%20Language/#protobuf","title":"protobuf","text":""},{"location":"ProtoBuf/ProtoBuf%20VS%20JSON%20VS%20XML/","title":"Protocol Buffers vs JSON vs XML","text":"<p>A comparison of different data serialization formats and their characteristics:</p> Feature Protocol Buffers ([[ProtoBuf]]) JSON (JavaScript Object Notation) XML (eXtensible Markup Language) Format Type Binary Text-based Text-based Performance (Speed) Very fast serialization/deserialization; up to 6x faster than JSON in server-to-server contexts. Slower than Protobuf due to text parsing overhead. Generally the slowest due to high verbosity and complex parsing rules. Performance (Size) Highly compact; significantly smaller payloads than JSON/XML. More compact than XML but larger than Protobuf due to text-based keys and syntax. Most verbose, resulting in the largest file sizes. Schema Requirement Strictly required; defined in a .proto file. Enforces a strong contract. Schema-less by default; schema can be added via external standards like JSON Schema or OpenAPI. Schema is optional (DTD, XSD) but commonly used in enterprise systems. Human Readability Not human-readable; requires tools for inspection. Human-readable and easy to inspect, aiding in debugging. Human-readable, though verbosity can make it complex to parse visually. Data Type Support Rich set of scalar types, including various integer sizes and encodings; supports enums and complex nested messages. Limited data types (string, number, boolean, array, object, null). Supports a wide range of data types through schema definitions (XSD). Compatibility Excellent backward and forward compatibility features built into the protocol design. No built-in compatibility features; relies on application-level conventions. Schema evolution is possible but can be complex to manage. Primary Use Cases High-performance internal microservices (gRPC), IoT, data storage, and performance-critical systems. Public web APIs, client-side web applications, configuration files, and scenarios valuing simplicity and readability. Enterprise systems, legacy applications, complex document formats, and standards like SOAP. <p>Example Payload for different format</p> Format Payload Example Size (bytes) Notes JSON {\"id\":42} 9 bytes Assumes ASCII, no whitespace. XML 42 11 bytes Assumes ASCII, no whitespace. Protob 0x08 0x2a 2 bytes Schema-dependent, not human-readable. <p>Payload size difference between JSON and Protobuf</p> Payload Raw JSON Size Protobuf Size Protobuf Size (%) 0 tickers 58 bytes 13 bytes 22.4% 10 tickers 396 bytes 133 bytes 33.6% 200 tickers 6,578 bytes 2,413 bytes 36.7% 2000 tickers 65,250 bytes 24,013 bytes 36.8% <p>#protobuf #preformance</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/","title":"Protocol Buffer Workflow","text":"<p>[[ProtoBuf]] workflow fundamentally shifts development from a \"code-first\" to a \"contract-first\" paradigm, a change with significant implications for team collaboration and system architecture.</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/#the-proto-file","title":"The .proto File","text":"<p>The entire Protocol Buffer workflow originates from a single artifact: the <code>.proto</code> file. This plain text file serves as the definitive contract, or schema, for the data being exchanged. It contains language-agnostic definitions of data structures (known as messages) and optionally, the interfaces for remote services (known as services).</p> <p>This file is the central point of agreement between different components of a distributed system, such as a client and a server. It is managed as a source code asset, version-controlled, and serves as the single source of truth for the API contract.</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/#key-components","title":"Key Components","text":"<p>The basic structure of a <code>.proto</code> file includes several key components:</p> <ol> <li> <p>Syntax/Edition Declaration: The first non-comment line must declare the version of the Protocol Buffer language being used, for example:    <pre><code>syntax = \"proto3\";\n</code></pre>    or    <pre><code>edition = \"2023\";\n</code></pre></p> </li> <li> <p>Package Declaration: A package declaration helps prevent naming conflicts between different projects and provides a namespace:    <pre><code>package tutorial;\n</code></pre></p> </li> <li> <p>Options: Language-specific option directives can be used to control how code is generated. For instance:    <pre><code>option java_package = \"com.example.tutorial\";\n</code></pre>    This specifies the Java package for the generated classes.</p> </li> <li> <p>Message Definitions: The core of the file, where message blocks define the structure of the data, including field names, types, and unique numbers.</p> </li> </ol> <p>By centralizing the API definition in this way, the .proto file becomes the focal point for design reviews and collaboration. Teams can agree on the contract before writing any implementation code, which decouples the interface from the implementation and facilitates parallel development. This \"contract-first\" approach significantly reduces integration errors and creates a system that is inherently self-documenting, with the .proto file acting as formal, machine-readable documentation.</p>"},{"location":"ProtoBuf/ProtoBuf%20Workflow/#protobuf","title":"protobuf","text":""},{"location":"ProtoBuf/ProtoBuf/","title":"ProtoBuf","text":"<p>Originated within Google as an internal project. The primary motivation was to create a more efficient and robust mechanism for communication between Google's vast number of internal services. Full form is \"Protocol Buffers\".</p> <p>XML was a common choice for data interchange but its performance characteristics were inadequate for scale and latency requirement of Google's infrastructure. JSON gained popularity for its simplicity and direct mapping to JavaScript objects, but as a text-based format, it still carries significant size and parsing overhead compared to optimized binary formats.</p> <p>All data structure in protobuf are called messages. </p>"},{"location":"ProtoBuf/ProtoBuf/#core-philosophy","title":"Core Philosophy","text":"<p>The design of Protocol Buffer is built upon a distinct philosophy that prioritizes machine-to-machine efficiency and contract-based design over given readability and ad-hoc flexibility. This philosophy is embodied in three core principles:</p> <ol> <li> <p>Schema-Driven: Messages must be explicitly defined in special files with a <code>.protobuf</code> extension. These files are written using [[Interface Definition Language]] that describes the fields of a message, their data type, and their unique identifying numbers. This schema-first approach enforces a strict contract between communicating parties, ensuring type safety and data consistency. This contract becomes the single source of truth for the data structure. More details are present in [[ProtoBuf Workflow]].</p> </li> <li> <p>Language-Agnostic and Platform-Neutral: The <code>.proto</code> file serves as a universal, language-independent contract. From this single definition, the [[Protoc Compiler|Protobuf compiler]] (<code>protoc</code>) generates native source code for a wide array of programming languages, including C++, Java, Python, Go, C#, Dart, and Ruby. The generated code provides a strongly-typed, easy-to-use API for creating, manipulating, serializing, and deserializing the defined messages. This allows services written in different languages to communicate seamlessly, as long as they are compiled from the same <code>.proto</code> definition.</p> </li> <li> <p>Performance-Oriented: Design is optimized for speed and compactness. The format is a dense binary representation which is significantly smaller than the equivalent text-based formats like JSON or XML. Instead of using human-readable fields in the payload, Protobuf uses unique integer tags to identify fields, further reducing size. It employs efficient encoding techniques such as variable-length integers to represent numbers compactly. The combination of these techniques results in smaller payloads which consume less of everything and results in faster [[Serializing and Deserializing in ProtoBuf]].</p> </li> </ol>"},{"location":"ProtoBuf/ProtoBuf/#evolution-of-protobuf","title":"Evolution of Protobuf","text":"Feature proto2 Behavior proto3 Behavior edition = \"2023\" Behavior Syntax Declaration syntax = \"proto2\"; (or omitted) syntax = \"proto3\"; edition = \"2023\"; Field Cardinality required, optional, repeated Singular (implicitly optional), repeated Singular (explicit presence by default), repeated required Keyword Supported, but strongly discouraged for new designs. Removed. Use of required is a compile-time error. Not supported. Migrated required fields use a special feature set. Field Presence Explicit presence for optional fields. A has_ method is generated to check if a field was set. Implicit presence for scalar fields by default (cannot distinguish unset from default value). optional keyword was later added to enable explicit presence. Explicit presence is the default for all singular fields, providing clear tracking of whether a field is set. Default Values Custom default values can be specified using [default = value]. Custom default values are not supported. Fields default to a type-specific zero value (0, empty string, false). Explicit default values are reintroduced. Enums The first enum value can be non-zero. The first enum value must be zero, which serves as the default. The first enum value must be zero. Extensions Natively supported as a core feature. Not supported (though Any provides an alternative). Reintroduced as a feature."},{"location":"ProtoBuf/ProtoBuf/#protobuf","title":"protobuf","text":""},{"location":"ProtoBuf/Protoc%20Compiler/","title":"Protoc Compiler","text":"<p>Once the <code>.proto</code> schema is defined, the next step is to use the Protocol Buffer compiler, <code>protoc</code>. This is a command-line tool that parses the <code>.proto</code> file and generates source code in a specified target programming language. This generated code provides the necessary classes, data structures, and methods to work with the defined messages in a native, idiomatic way for that language.</p> <p>The <code>protoc</code> compiler is designed with a highly extensible plugin architecture. The core <code>protoc</code> binary is responsible for parsing the <code>.proto</code> file into an abstract syntax tree. It then passes this representation to a language-specific plugin, which is responsible for the actual code generation.</p> <p>A typical invocation of the compiler from the command line looks like this:</p> <pre><code>protoc -I=./protos --java_out=./src/main/java./protos/my_message.proto\n</code></pre> <p>The key flags are:</p> <ul> <li> <p><code>-I</code> or <code>--proto_path</code>: Specifies the directory in which to search for <code>.proto</code> files and their imports. Multiple paths can be provided.</p> </li> <li> <p><code>--&lt;lang&gt;_out</code>: Specifies the output directory for the generated code in the target language (e.g., <code>--java_out</code>, <code>--go_out</code>, <code>--python_out</code>).</p> </li> </ul> <p>This generates a set of source files that provide rich, language-specific APIs for the messages defined in <code>.proto</code> file. The generated code typically includes:</p> <ol> <li>Message Classes/Structs: A class or struct for each message definition, with properties or fields corresponding to those in the <code>.proto</code> file.</li> <li>Accessors: Getters and setters for each field, allowing for easy and type-safe manipulation of the data.</li> <li>Builders/Factories: Fluent builder patterns or factory methods for constructing message instances in a readable and convenient way (e.g., <code>Person.newBuilder().setName(\"John\").setId(123).build()</code>).</li> <li>Presence Methods: For fields with explicit presence (optional fields in proto2 or proto3, or all singular fields in editions), methods are generated to check if a field has been set (e.g., <code>has_name()</code>).</li> <li>Serialization Methods: Functions to serialize the message object into a compact binary byte array (e.g., <code>SerializeToString()</code>, <code>toByteArray()</code>, <code>writeTo(OutputStream)</code>).</li> <li>Deserialization Methods: Static methods or functions to parse a binary byte array back into a message object (e.g., <code>ParseFromString()</code>, <code>parseFrom(InputStream)</code>).</li> </ol>"},{"location":"ProtoBuf/Protoc%20Compiler/#protobuf","title":"protobuf","text":""},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/","title":"Serializing and Deserializing in Protocol Buffers","text":""},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/#serializing-messages","title":"Serializing Messages","text":"<p>The process of serializing a message typically involves three steps:</p> <ol> <li>Instantiation</li> <li>Create an instance of the generated message class</li> <li>Often uses a builder pattern for clarity</li> <li> <p>Ensures the object is immutable once created</p> </li> <li> <p>Population</p> </li> <li>Use the generated setter methods</li> <li> <p>Populate the fields of the message with application data</p> </li> <li> <p>Serialization</p> </li> <li>Call one of the serialization methods (e.g., <code>SerializeToString()</code> or <code>toByteArray()</code>)</li> <li>Executes highly optimized serialization logic</li> <li>Converts the in-memory object into a compact byte array</li> <li>Ready for transmission or storage</li> </ol>"},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/#binary-format","title":"Binary Format","text":"<p>The resulting binary data is not self-describing: - Stream of key-value pairs - \"Key\" is a combination of:   - Field number   - Wire type identifier - \"Value\" is the encoded data payload - Field names and precise data types are not included</p> <p>This structure is a fundamental reason for [[Serializing and Deserializing in ProtoBuf|Protobuf's efficiency]], but it requires the exact same schema definition (or the code generated from it) to correctly interpret the data on the receiving end.</p>"},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/#deserializing-messages","title":"Deserializing Messages","text":"<p>The deserialization process is the mirror image of serialization:</p> <ol> <li>Receive Data</li> <li>Obtain the serialized byte array</li> <li> <p>Can come from:</p> <ul> <li>Network socket</li> <li>File</li> <li>Other data sources</li> </ul> </li> <li> <p>Parsing</p> </li> <li>Call a static parsing method on the corresponding generated message class</li> <li>Pass the byte array as an argument</li> <li> <p>Example: <code>Person.parseFrom(data)</code></p> </li> <li> <p>Usage</p> </li> <li>Returns a fully populated, read-only instance of the message object</li> <li>Use the generated getter methods to access the data</li> <li>Object is immutable to ensure thread safety</li> </ol>"},{"location":"ProtoBuf/Serializing%20and%20Deserializing%20in%20ProtoBuf/#protobuf","title":"protobuf","text":""},{"location":"gRPC/Foundation%20of%20gRPC/","title":"Foundation of gRPC","text":"<p>gRPC is a \"contract-first\" philosophy, which is enforced by its default [[Interface Definition Language|Interface Definition Language (IDL)]], [[ProtoBuf | protocol buffers]].</p> <p>It uses protobuf by default for both its IDL for defining service contracts and underlying message interchange format for serialization. </p>"},{"location":"gRPC/Foundation%20of%20gRPC/#contract-first-philosophy","title":"Contract first philosophy","text":"<p>Many REST-based architectures, the API contract (such as an OpenAPI/Swagger specification) is often treated as an afterthought. It is frequently written after the code is implemented, simply to document what has been built. This \"code-first\" approach inevitably leads to \"schema drift,\" where the documentation and the implementation become out of sync, causing integration failures.</p> <p>The gRPC workflow  makes this class of error impossible. The .proto file  is the single, indisputable source of truth. This \"strongly typed contract\" catches data structure and service errors at compile-time, not at runtime.</p>"},{"location":"gRPC/Foundation%20of%20gRPC/#devlopers-workflow","title":"Devlopers workflow","text":"<p>Read [[ProtoBuf Workflow]] , [[Protoc Compiler]] and [[Serializing and Deserializing in ProtoBuf]] might be helpful in understanding the complexity beneath this.</p> <p>The .proto file is the starting point for a rigid, automated development workflow that is central to gRPC's design.</p> <ol> <li> <p>Step 1: Define: The developer authors the .proto file, defining all data message types and service RPCs. This file becomes the single, authoritative source of truth for the API contract.</p> </li> <li> <p>Step 2: Compile: The developer runs the Protocol Buffer compiler, protoc. This compiler is used with a language-specific gRPC plugin (e.g., protoc-gen-go-grpc).</p> </li> <li> <p>Step 3: Generate: The compiler generates source code in the target programming language. This code is not trivial; it is a comprehensive set of tools, including     </p> <ul> <li> <p>Data Access Classes: For each message type, the compiler generates a class (e.g., HelloRequest in Java or C++) with all the necessary methods for populating fields, serializing the message to binary, and parsing binary data back into the object.</p> </li> <li> <p>Server Skeleton: A \"service base class\" or interface (e.g., GreeterServer) that defines the abstract methods the server-side application must implement.</p> </li> <li> <p>Client Stub: A client-side \"stub\" (e.g., GreeterClient) that provides concrete methods for the client application to call. This stub handles all the work of serializing the request, sending it to the server, and parsing the response.</p> </li> </ul> </li> <li> <p>Step 4: Implement: The server-side developer's task is reduced to implementing the generated service interface. The client-side developer's task is reduced to instantiating and calling the methods on the generated client stub.</p> </li> </ol>"},{"location":"gRPC/Foundation%20of%20gRPC/#grpc","title":"grpc","text":""},{"location":"gRPC/gRPC/","title":"gRPC","text":"<p>It is highly recommend to go through the [[docs/ProtoBuf/ProtoBuf|ProtoBuf]] module before reading gRPC</p>"},{"location":"gRPC/gRPC/#introduction","title":"Introduction","text":"<p>gRPC originally stood for Google Remote Procedure Call. It is a modern, open-source, high-performance remote procedure call framework. It is designed to operate in any environment. It is especially good for servers-to-servers communication, which may be within or across data centers. This applicability extends to the last mile of the distributed computing which serves a wide range of clients which include IoT devices, web browsers, mobile applications, etc.</p> <p>The [[Foundation of gRPC|fundamental concept of gRPC]] is built upon the established paradigm of RPC. It allows client applications to directly invoke a method on a server application which may be on a different machine or in a different data center as if it were a local object or a function call. This abstraction is the corner store of distributed systems as it makes the underlying complexity of network communication, data serialization, and error handling thereby simplifying the development of distributed applications and services.</p>"},{"location":"gRPC/gRPC/#how-it-came-into-being","title":"How it came into being","text":"<p>It was an internal project of google called stubby. its powered the core of vast and planet scale infrastructure. It was built to manage the communication layer for system that handle 10s of billion request per second. </p> <p>This internal system provided Google with the extreme scalability, performance, and cross-language functionality required to operate its services.</p> <p>Google made this project open-source in 2015. This release effectively democratized access to a level of distributed systems technology that was previously only available within hyperscale companies.</p> <p>The primary design goals for this project were:</p> <ul> <li> <p>Polyglot Environments: How to make services written in C++, Java, and Python communicate seamlessly.</p> </li> <li> <p>Strict Contracts: How to enforce rigid service contracts to prevent \"brittle\" integrations when thousands of teams iterate independently.</p> </li> <li> <p>High-Performance Serialization: How to minimize the network footprint and CPU cost of serializing and deserializing data billions of times a second.</p> </li> </ul>"},{"location":"gRPC/gRPC/#grpc","title":"grpc","text":""}]}